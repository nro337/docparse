{
  "papers": [
    {
      "id": 1,
      "url": "https://www.nature.com/articles/s41746-025-02022-1",
      "title": "A generative AI teaching assistant for personalized learning in medical education",
      "abstract": "Medical education faces a scalability crisis, where rising class sizes strain individualized instruction, while students increasingly adopt unvalidated Generative AI (GenAI) tools for individualized learning support. This study investigated how medical students integrate constrained GenAI systems into their self-directed learning practices using Retrieval-Augmented Generation (RAG), which limits large language model responses to instructor-curated materials, thereby reducing hallucinations while maintaining pedagogical utility. We deployed a RAG-based teaching assistant in a medical school basic science course across two consecutive cohorts, examining usage patterns, conversation content, and student feedback to understand adoption and learning behaviors. Students demonstrated strategic, context-dependent usage, with engagement intensifying during high-stakes assessment periods and substantial after-hours utilization. Users primarily sought clarification on foundational concepts and valued the system's continuous availability and source-grounded responses. However, knowledge-base constraints that ensured accuracy also limited broader inquiries, creating tension between reliability and comprehensiveness that shaped how students incorporated the tool into their study routines. These findings provide empirical evidence of how medical students navigate constrained AI tools for self-directed learning, informing institutional strategies for integrating these technologies into pedagogical frameworks.",
      "added_date": "2025-12-21T19:06:42.906817",
      "markdown": "# A generative AI teaching assistant for personalized learning in medical education\n\n- [Thomas Thesen](#auth-Thomas-Thesen-Aff1-Aff2) [1](#Aff1) , [2](#Aff2) &amp;\n- [Soo Hwan Park](#auth-Soo_Hwan-Park-Aff3) [3](#Aff3)\n\n[*npj Digital Medicine*](/npjdigitalmed) **volume 8** , Article number: 627 ( 2025 ) [Cite this article](#citeas)\n\n- 11k Accesses\n- 2 Citations\n- 60 Altmetric\n- [Metrics details](/articles/s41746-025-02022-1/metrics)\n\n### Subjects\n\n- [Computational biology and bioinformatics](/subjects/computational-biology-and-bioinformatics)\n- [Mathematics and computing](/subjects/mathematics-and-computing)\n- [Neuroscience](/subjects/neuroscience)\n- [Psychology](/subjects/psychology)\n\n## Abstract\n\nMedical education faces a scalability crisis, where rising class sizes strain individualized instruction, while students increasingly adopt unvalidated Generative AI (GenAI) tools for individualized learning support. This study investigated how medical students integrate constrained GenAI systems into their self-directed learning practices using Retrieval-Augmented Generation (RAG), which limits large language model responses to instructor-curated materials, thereby reducing hallucinations while maintaining pedagogical utility. We deployed a RAG-based teaching assistant in a medical school basic science course across two consecutive cohorts, examining usage patterns, conversation content, and student feedback to understand adoption and learning behaviors. Students demonstrated strategic, context-dependent usage, with engagement intensifying during high-stakes assessment periods and substantial after-hours utilization. Users primarily sought clarification on foundational concepts and valued the system's continuous availability and source-grounded responses. However, knowledge-base constraints that ensured accuracy also limited broader inquiries, creating tension between reliability and comprehensiveness that shaped how students incorporated the tool into their study routines. These findings provide empirical evidence of how medical students navigate constrained AI tools for self-directed learning, informing institutional strategies for integrating these technologies into pedagogical frameworks.\n\n## Introduction\n\nGenerative AI has the potential to provide personalized learning opportunities in medical education. Rising class sizes in medical schools, coupled with increasingly diverse student learning needs, create environments where individualized attention becomes increasingly difficult to provide. At the same time, didactic courses struggle to accommodate the varied pacing and comprehension levels of students, particularly in content-heavy subjects like organ-system=based preclerkship courses. This issue is further complicated by the limited availability of faculty for individual clarification and support outside standard business hours, a time when many students study [1](/articles/s41746-025-02022-1#ref-CR1) , [2](/articles/s41746-025-02022-1#ref-CR2) .\n\nAI-driven platforms have the potential to tailor instruction to individual students' weaknesses and provide immediate custom feedback. The educational benefit of personalized instruction is well-established. Studies show that reducing student-to-teacher ratios significantly improves learning outcomes [3](/articles/s41746-025-02022-1#ref-CR3) , [4](/articles/s41746-025-02022-1#ref-CR4) , and one-on-one tutoring can substantially enhance student performance [5](/articles/s41746-025-02022-1#ref-CR5) . While peer-tutoring programs address some of these concerns, providing each medical student with individual human tutoring across all medical school courses remains impractical at scale. And today's medical students, as digital natives, increasingly turn to online resources for immediate, interactive learning support [6](/articles/s41746-025-02022-1#ref-CR6) . In fact, with the wide commercialization of generative artificial intelligence (GenAI), surveys indicate that approximately half of medical students use large language model (LLM) chatbots, such as ChatGPT, during their studies, with many engaging weekly or more for learning and writing assistance [7](/articles/s41746-025-02022-1#ref-CR7) . Notably, students often prefer asking LLMs questions over consulting textbooks or instructors, driven by the appeal of instant, personalized responses. This represents a fundamental shift toward digital, on-demand learning tools in medical education [8](/articles/s41746-025-02022-1#ref-CR8) , [9](/articles/s41746-025-02022-1#ref-CR9) .\n\nHowever, this technological adoption comes with specific challenges. LLMs, while powerful and eager in generating helpful explanations, are prone to producing incorrect or fabricated information, so-called \"hallucinations\" [10](/articles/s41746-025-02022-1#ref-CR10) . This poses major challenges for their practical application in medical training, where content accuracy and alignment with curriculum standards and current best-practices in medicine are critically important. Retrieval-augmented generation (RAG) offers a promising solution to address these accuracy concerns. RAG is a hybrid approach that combines the generative capabilities of LLMs with a retrieval mechanism that searches through a curated knowledge base. When a user asks a question, the system first retrieves relevant information from a specific database (in this case, course materials), then provides this retrieved content as context to the LLM, which generates a response grounded in these authoritative sources rather than relying solely on its pre-trained knowledge [11](/articles/s41746-025-02022-1#ref-CR11) . By constraining LLM responses to instructor-curated, course-specific materials rather than allowing unconstrained generation from general training data that may contain unverified or outdated medical information from the internet or old textbooks, RAG significantly reduces the risk of hallucinations while maintaining the conversational flexibility that makes LLMs valuable as educational tools. This approach has shown success in varied business and educational settings for reducing hallucinations and increasing response relevance and accuracy [12](/articles/s41746-025-02022-1#ref-CR12) , [13](/articles/s41746-025-02022-1#ref-CR13) (See Fig. [1](/articles/s41746-025-02022-1#Fig1) ).\n\nFig. 1: NeuroBot TA System Architecture and Information Flow.\n\n<!-- image -->\n\nDespite RAG's promise in delivery of accurate information, it is unclear how well this technology can be integrated into medical education given that medical students represent a distinct adult-learner population with specialized learning needs. The cognitive demands of processing large amounts of complex information in compressed timeframes, coupled with medical students' established habits of utilizing multiple learning resources create unique implementation and adoption challenges for AI learning tools in medical education [14](/articles/s41746-025-02022-1#ref-CR14) .\n\nThe Technology Acceptance Model (TAM) [15](/articles/s41746-025-02022-1#ref-CR15) , [16](/articles/s41746-025-02022-1#ref-CR16) posits that perceived usefulness and ease of use adoption determine system usage, and numerous studies have investigated student adoption of learning technologies using this framework. Students tend to embrace tools they perceive as both beneficial to their learning goals and straightforward to integrate into existing study practices [17](/articles/s41746-025-02022-1#ref-CR17) . Following this framework, we developed the following research questions: (1) How would medical students integrate an AI teaching assistant into their self-regulated learning processes? (2) Would the RAG-based design enhance perceived usefulness by increasing trust in AI-generated answers?, and (3) Would the conversational interface and 24/7 availability improve perceived ease of use compared to traditional resources? Drawing from TAM, we hypothesized that students would adopt NeuroBot TA if they perceived it as enhancing their learning efficiency, i.e., usefulness, while requiring minimal effort to learn and operate, i.e., ease of use, and that usage patterns would correspond to periods of high information need when the added value of the system was most evident. We expected this strategic, exam-focused adoption pattern based on cognitive load theory that predict students preferentially seek external support when cognitive demands peak and when time constraints make efficient clarification of uncertainties most valuable [18](/articles/s41746-025-02022-1#ref-CR18) .\n\nWhile there has been a substantial interest in deploying LLMs for medical education, to our knowledge, this is the first study that reports on the deployment and evaluation of usage patterns and student attitudes towards a RAG-based LLM platform in medical education. The only deployed, corpus-grounded system has been used in a graduate medical education journal club that embedded assigned articles in a vector database and was evaluated qualitatively with residents and faculty [19](/articles/s41746-025-02022-1#ref-CR19) . By contrast, most work to date has relied on base LLMs without automated and constrained retrieval that range from clerkship feedback tools and ward-based case studies to co-designed tutoring concepts, but not employing provenance-grounded retrieval [20](#ref-CR20) , [21](#ref-CR21) , [22](#ref-CR22) , [23](/articles/s41746-025-02022-1#ref-CR23) .\n\n## Results\n\n### Usage and engagement statistics\n\nAcross two academic years, students initiated 360 unique conversations with NeuroBot TA, generating a total of 2946 individual messages. See Fig. [2](/articles/s41746-025-02022-1#Fig2) for usage analytics and engagement patterns. Conversation length ranged from 1 to 47 turns (with 1 turn being defined as one new message by either a student or the assistant within a conversation) with a mean of 3.6 turns per conversation (Median\u2009=\u20092). The distribution of conversation turns is shown in Fig. [2A](/articles/s41746-025-02022-1#Fig2) . Student message length ranged from 1 to 272 words (median: seven words, SD\u2009=\u200920.65) while bot responses ranged from 2 to 1338 words (median: 82 words, SD\u2009=\u2009130).\n\nFig. 2: Usage Analytics and Engagement Patterns.\n\n<!-- image -->\n\nTemporal analysis of weekly interaction patterns showed that usage peaked mid-week, with lowest engagement on Fridays and Saturdays (Fig. [2B](/articles/s41746-025-02022-1#Fig2) ). Between academic years, total number of conversations decreased from 256 to 104 (\u221259.4%) and number of messages from 1922 to 1024 (\u221246.7%).\n\n### Time-of-day analysis\n\nDaily patterns showed highest use during the day but also substantial after-hours utilization (i.e., after 5\u2009pm) (Fig. [2A](/articles/s41746-025-02022-1#Fig2) ). Timeseries plots of conversation frequency from both cohorts showed large increases in the days leading up to exams (Fig. [3B, D](/articles/s41746-025-02022-1#Fig3) ). While students in cohort 1 engaged with NeuroBot TA consistently throughout the course, usage by students in cohort two dropped significantly throughout the course. Across both cohorts, independent samples t-tests comparing conversation volume during pre-exam periods (defined as the three days leading up to each exam, *M* =\u20095.11, SEM\u2009=\u20091.00, *n* =\u200918) with regular course periods ( *M* =\u20091.14, SEM\u2009=\u20090.14, *n* =\u2009236) showed a significant increase during pre-exam periods compared to regular course periods (t(252)\u2009=\u20093.95, *p* &lt;\u20090.001, Cohen's *d* =\u20091.74), overall representing a 329.6% increase in conversation volume during the three days leading up to exams. Sub-analysis of the 2023-2024 academic year showed that conversation frequency increased during pre-exam periods (7.33\u2009\u00b1\u20091.22 conversations/day) compared to average frequency across the whole period (1.58\u2009\u00b1\u20090.23 conversations/day), representing a 363.2% increase ( *t* =\u20094.616, *p* =\u20090.0014, Cohen's *d* =\u20092.232). For 2024-2025, a similar pattern emerged with pre-exam periods averaging 2.89\u2009\u00b1\u20091.22 conversations/day versus 0.67\u2009\u00b1\u20090.13 during regular periods (329.6% increase), though this difference did not reach statistical significance ( *t* =\u20091.809, *p* =\u20090.1073, Cohen's *d* =\u20091.344). The results of the differences in conversation volumes are shown in Fig. [3C](/articles/s41746-025-02022-1#Fig3) . Two-way ANOVA confirmed significant main effects for both conditions (pre-exam vs. regular, *p* &lt;\u20090.001) and academic year ( *p* &lt;\u20090.001), with a significant interaction effect ( *p* =\u20090.0010), indicating that the first cohort engaged more with the TA bot for exam preparation.\n\nFig. 3: Usage Patterns and Exam Timing Analysis.\n\n<!-- image -->\n\n### Conversation content analysis\n\nThematic analysis of student messages to the chatbot identified eight primary content domains (Fig. [4A](/articles/s41746-025-02022-1#Fig4) ). Neuroanatomy and physiology constituted the most frequent theme (65.9%, 216 conversations, examples: *tell me about the vermis of the cerebellum; what do the mamillary bodies do?)* , followed by clinical syndromes and disorders (53.7%, 176 conversations, examples: *what is Brown-Sequard syndrome?; What causes ALS?* ). Others included educational methods and resources (31.4%, 103 conversations, examples: *what is the course grade composed of?; Give me some high yield information for benchmark #2* ), Neural Pathways and Tracts (29.6%, 97 conversations, examples: *What is the pathway for touch?; Is the dorsal column medial lemniscus pathway myelinated?* ), and Course and Exam Information (28.4%, 93 conversations). Less frequent topics included Pharmacology and Treatment (14.9%, 49 conversations, examples: *What do I need to know about trihexyphenidyl?; What are the side effects of carbidopa?* ), Clinical Case Discussions (14.3%, 47 conversations, examples: *What are the differential diagnosis for a patient with pain in the left eye?; A 23-year-old-woman is brought to the emergency room after being involved in a car crash...* ), and Imaging and Diagnostic Techniques (7.0%, 23 conversations, examples: *what would parkinsons show on mri?; what is the 'hot cross bun' sign on mri?* ). Percentages do not add to 100% because a conversation could include multiple themes. On the independently human-coded validation subset, overall LLM-human percent agreement was 78.4%. Weighted Cohen's \u03ba for the ordered 0-3 intensity ratings was 0.64 (95% bootstrap CI 0.58-0.70). When collapsing ratings to present vs absent, overall macro-F1 was 0.76, together showing moderate to good alignment with human coding.\n\nFig. 4: Distribution of Primary Themes in Conversations.\n\n<!-- image -->\n\n### Student feedback\n\nIn the first cohort (2023-2024), 39.3% (22/56) of students who answered the end-of-course survey reported using the AI assistant at least once during the course, while usefulness was judged at 2.8/5 (SD\u2009=\u20091.4). In the second cohort (2024-25), 26.4% (23/87) of students who completed the survey reported using NeuroBot TA at least once, with usefulness judged as 3.3/5 (SD\u2009=\u20091.0). Overall usage across both cohorts was 31.4%.\n\nFigure [4B](/articles/s41746-025-02022-1#Fig4) shows eight primary themes and their frequency identified in student feedback comments. The 'Variable Helpfulness' theme was the most prevalent theme (68.4%, 13 comments, example: *\"I tried to have the Bot come up with a study guide off of the red starred slides and it wasn't able to, but it could give me feedback on grading breakdowns.\")* , followed by the 'Supplementary Learning Aid' theme (47.4%, 9 comments, example: *\"Was extremely useful to be able to quickly ask the AI if a question came up while I was preparing for an exam.\"* ), and the 'Limited Scope' theme (36.8%, 7 comments, example: *\"wasnt helpful. I use other AI tools to help create comparative/summary tabels and this was very helpful, but Neurobot ta wasn't able to answer a lot of questions and wasn't that helpful* \"). Less frequent themes included 'Convenience and Integration' (26.3%, 5 comments, example: \" *is pretty good at summarizing info instead of going through lots of slides for a specific question* \"), 'Trust and Reliability' (26.3%, 5 comments, example: *\"I don't trust AI yet to give me learning materials, especially after having tried Chat GPT with research articles. I'm aware that the NeuroBot TA only pulls from class materials, which is great.\")* , and Prompt Sensitivity (10.5%, 2 comments, example: *\"The NeuroBot would often give very long answers for relatively simple questions. I don't blame the bot for this though, I was probably using inefficient prompts and am someone who is still in the habit of searching the web for information rather than using a chatbot.\"* ).\n\n## Discussion\n\nThis study examined the implementation of a RAG-based AI teaching assistant in medical education across two consecutive academic cohorts and demonstrates a scalable, always-available AI support tailored to curricular contexts and students needs. The results identified both opportunities and challenges in deploying RAG-based AI teaching assistants in medical education programs.\n\nThe data from participating students demonstrate how some medical students integrated NeuroBot TA into their self-regulated learning processes. Students exhibited strategic, context-dependent usage patterns rather than continuous engagement. The 329% surge in usage during pre-exam periods and substantial after-hours utilization (post-5\u2009pm) indicate that students primarily leveraged the system as a just-in-time learning resource during intensive study sessions that often extended beyond the hours of instructor availability. The average conversation length of 3.6 turns suggests focused, targeted queries rather than extended tutoring sessions. This pattern indicates that medical students use the chatbot as a targeted reference tool during self-directed study, asking specific questions to clarify concepts rather than seeking extended tutoring or comprehensive instruction.\n\nThe RAG-based design showed mixed results in enhancing perceived usefulness through increased trust. Positive indicators included students' appreciation for source citations (26.3% of feedback comments mentioned trust and reliability themes), with one student noting that knowing \"the NeuroBot TA only pulls from class materials\" increased confidence. The predominance of curriculum-aligned queries (66% neuroanatomy, 54% clinical disorders) suggests students trusted the system for course-specific content. However, the modest usefulness ratings (2.8/5 and 3.3/5 across cohorts) and the \"limited scope\" frustration theme (36.8% of comments) indicate that while restricting responses to content in the course materials may enhanced accuracy and trust, it simultaneously reduced perceived utility of the tool by restricting response breadth, thereby creating a tension between response reliability and comprehensiveness.\n\nThe conversational interface and 24/7 availability did improve perceived ease of use, as evidenced by usage patterns and feedback. Some users valued the \"convenience and integration\" (26.3% of feedback), with comments highlighting quick access to answers during exam preparation. The temporal distribution showing after-hours usage and mid-week engagement peaks suggest that users found the system accessible when needed. However, some students noted challenges with prompt engineering (10.5% mentioned prompt sensitivity), suggesting that while the interface was accessible, optimal utilization may have required AI skills students were still developing.\n\nThese findings collectively support our TAM-based hypothesis partially: students adopted NeuroBot TA when perceived benefits were most salient (pre-exam periods), but overall adoption remained moderate (31.4%) due to the tension between the system's reliability constraints and students' expectations for comprehensive support.\n\nContent analysis of the chat transcripts confirmed that students primarily used NeuroBot TA to reinforce core course knowledge. The vast majority of queries centered on the course's fundamental content and neuroanatomy and neurophysiology concepts were the single largest topic, appearing in about 66% of conversations, and clinical neurological disorders appeared in ~54%. Students also frequently asked about study resources or clarifications of course logistics (appearing in roughly 28-31% of chats, e.g., exam information or \"high-yield\" review tips). Other academic topics like neural pathways, tract anatomy, and pharmacology were moderately represented, whereas more complex clinical case discussions or neuroimaging interpretation were relatively infrequent. This distribution suggests that the AI assistant was mostly used for fact-based clarification and review of taught material, rather than for open-ended clinical reasoning practice. It may also reflect the constraints of its knowledgebase as NeuroBot TA was intentionally limited to instructor-curated course materials, which ensured answers stayed aligned with the curriculum but inherently capped the scope of questions it could address. The content pattern therefore illustrates a trade-off where the bot excelled at fielding questions on covered topics, but students may have recognized that queries outside the provided content, or requiring extensive synthesis, were beyond its scope.\n\nStudent feedback on NeuroBot TA highlighted both the promise of AI support as well as opportunities for improvement. Qualitative comments showed that while many appreciated the bot as a convenient supplemental resource, its performance was inconsistently helpful. The most prevalent theme in feedback was \"variable helpfulness\" and students noted the AI could answer certain questions well (for example, providing quick explanations or grading clarifications) but failed at other tasks (such as generating comprehensive study guides). Similarly, nearly half of commenting students viewed NeuroBot as a \"supplementary learning aid\", valuing the ability to get instant answers during self-study (e.g., while preparing for exams). On the other hand, many users were frustrated by the system's \"limited scope\" and about 37% of comments noted that the bot could not address numerous questions, especially those requiring information beyond the uploaded course slides. This limitation sometimes led students to revert to other AI tools or resources for comparison. Additional feedback themes underscored practical considerations. For example, some praised the convenience and integration of the 24/7 chat format within the learning platform, noting it saved time searching through slides. However, trust and reliability emerged as a concern (26% of comments) where a subset of students expressed reluctance to fully trust any AI-generated answers without verification. Encouragingly, students acknowledged that constraining the bot to official class materials through RAG improved their trust. Lastly, some users mentioned that the quality of answers depended on how questions were asked, with the bot sometimes giving overly lengthy answers to simple questions. This indicates that students were still learning how to interact optimally with LLM chatbots, which is an expected challenge as both users and technology co-evolve.\n\nUsage declined notably in cohort 2, potentially reflecting the rapidly evolving GenAI landscape. When deployed with cohort 1 in Fall 2023, NeuroBot TA represented novel technology less than 10 months after ChatGPT's initial release. Internal survey data indicated limited prior AI experience among cohort 1 students, making the Neurobot TA system novel and distinctive. Conversely, cohort 2 in Fall 2024 entered during a period of more widespread AI adoption in higher education. Medical student utilization of general AI platforms increased dramatically during this period, with up to 89% reporting regular use [24](/articles/s41746-025-02022-1#ref-CR24) . While the present study design does not allow us to draw clear inferences, the growing availability of commercial alternative models with reasoning capabilities that are demonstrating improved performance on medical knowledge benchmarks may lead students to use these systems more frequently [25](/articles/s41746-025-02022-1#ref-CR25) , [26](/articles/s41746-025-02022-1#ref-CR26) . The usage patterns observed with NeuroBot TA seem to reflect this demand for quick, accessible support where students tended to use the assistant as a just-in-time tutor during intensive study periods (notably mid-week and pre-exam) when instant clarification of concepts was most valued.\n\nThe finding that conversation volume increased substantially during pre-exam periods demonstrates how assessment events can drive the use of self-directed learning resources, including AI chatbots. This behavior indicates that students viewed the system primarily as an optional review tool rather than a continuous learning partner and suggests a strategic but yet limited integration into their study practices. The course where the chatbot was deployed was the last organ-system course at the end of Year 2 in the preclerkship curriculum. At this point students will have already found their preferred study method and tools and are therefore less likely to engage in strong shifts in how they study and take the risk of adapting a new, untested study tool. This may have affected the moderate adoption rate despite the societal hype of Generative AI at the time. In the future, implementation of RAG-based AI tutor system may early in the medical school curriculum when students have not yet solidified their study approach may lead to higher adoption rates, and should include best prompting techniques for interacting effectively with AI-teaching assistants.\n\nThere was a clear hierarchy of student learning priorities through chatbot interactions, with core biomedical content dominating conversation frequency, followed by clinical disorders, a finding that matches the content and focus of the preclerkship course. Students also asked a significant number of questions about course organization and exams, highlighting the utility of RAG-supported LLMs in answering questions specific to an individual course based on information that was not part of the original LLM training data. To that end, students primarily leveraged the AI assistant for clarifying course content and reviewing concepts, treating it as an on-demand tutor for course-related content.\n\nOverall, students demonstrated moderate willingness to engage with the course-constrained AI tool, and those who did valued its instant and around-the-clock access to verified information. This availability complements the contemporary nature of how medical students study, as they often study at odd hours, already use digital resources like question banks and online tools, and value self-paced studying. NeuroBot TA provided an additional tool that fit naturally into these eclectic patterns as it was accessible on the same devices they already use and at any time in any location. Additionally, NeuroBot TA's retrieval-based design provides highly targeted, on-demand explanations that align with students' individual study goals (e.g., lecture-specific clarification before exams). Overall, the principles of personalization that LLM-based RAG chatbots offer are consistent with the emerging framework of *Precision Medical Education* that advocates for tailoring educational interventions to each learner's specific needs and context [27](/articles/s41746-025-02022-1#ref-CR27) , [28](/articles/s41746-025-02022-1#ref-CR28) .\n\nOur findings also align with TAM predictions and demonstrate how perceived usefulness and ease of use shaped NeuroBot TA adoption. The increase in usage during pre-exam periods reflects TAM's principle that perceived usefulness drives adoption when benefits are most salient. Neuroanatomy-focused conversations and positive responses to source citations demonstrate that curriculum alignment and verifiability enhanced perceived usefulness. Substantial after-hours utilization suggests that students found the system accessible when most convenient, satisfying TAM's ease of use dimension. Finally, frustration regarding knowledge scope limitations highlights TAM's expectation alignment principle. This principle predicts that when system capabilities do not match student expectations, student satisfaction decreases, underscoring the importance of clearly communicating system capabilities and boundaries in educational AI implementations.\n\nNotably, NeuroBot TA's intentional constraint to its knowledge base led to student frustration when it refused to address queries beyond the scope of the knowledge base. This compares directly to all-purpose commercial chatbots who typically provide plausible-sounding answers regardless of factual accuracy. Student comments help illustrate this point in the context of the evolving GenAI landscape and the frustration with restricted answer space. A *cohort 1* student, for example, noted, \"The chatbot is interesting to play around with but if I have a question, I tend to just pull up Google because it is convenient and is what I have done my whole life,\", highlighting the student's unfamiliarity with Generative AI chatbots and preference towards established strategies to access knowledge. In contrast, a *cohort 2* student stated, \" I use other AI tools to help create comparative/summary tabels (sic) and this was very helpful, but Neurobot ta wasn't able to answer a lot of questions.\" A year later, the cohort 2 student had already incorporated GenAI technologies into their study practices, but valued unconstrained answers even if the likelihood of factual incorrectness may be higher.\n\nPedagogically, educators can address this through explicit instructions in Generative AI uses and misuses and providing students with the knowledge how to responsibly navigate these new and ubiquitous tools for their learning. Technically, future work may focus on developing RAG-based systems that are able to generate flexible study schemes that matches a student's learning habits (e.g., automatic generation of lecture-specific tables or flash cards) and showing students how to most effectively initiate conversations with specific aims.\n\nWith only a subsection of students using NeuroBot TA and survey responses from a subset of students who reported usage, our findings do not represent the entire medical student population. Statements about student preferences and strategic adoption should be interpreted within this self-selected sample in mind. In addition, the per-conversation analysis could not distinguish whether usage patterns reflected typical behavior or were driven by a small number of \"super users,\" as we did not track individual user engagement. This limits the ability to generalize directly to the average student experience. Despite theoretical and empirical works demonstrating reduced hallucinations and increased relevance of RAG-constrained responses, the current study did not systematically assess response accuracy. While periodic informal review by the course director identified no critical issues requiring intervention, we cannot quantify accuracy improvements or definitively confirm that RAG constraints eliminated all hallucinations. Finally, the content analysis relied primarily on GPT-4o for thematic coding with human validation of only 15% of conversations for theme coherence rather than comprehensive accuracy assessment. This approach, while efficient, may have missed errors that the LLM could not identify. Lastly, the deployment of the tool at only a single medical school and restricts the study's generalizability across diverse institutional contexts with varying curricula, teaching methodologies, and student demographics.\n\nRAG-constrained LLM chatbots show potential as adjunct study tools for some medical students in self-directed learning contexts. However, they need to be deployed thoughtfully in order to engage students meaningfully and contribute to their learning. We recommend that educators introduce the new technology early in the medical school curriculum, preferably in the first course when students are trying different study techniques that work for them in medical school. Educators who are considering broader implementation should focus foremost on carefully curated content that is submitted to the vector database to improve response relevance. Specific documents that contain useful information about the course resources, assessments and effective study strategies should be included to allow the bot to answer questions in this area, as we found that students asked these questions frequently. Bot responses should also enable a deeper dive into the material by highlighting the text sources that were retrieved and to link directly to the document of origin and the place of quotation. Furthermore, educators need to decide whether to restrict the bot to answer questions solely based on course material at the risk of student frustration or to allow increasingly sophisticated models to answer questions beyond immediate course content. Educators also need to communicate system capabilities and limitations clearly to help students select and leverage these tools effectively in their courses. Lastly, medical programs should ensure students gain fundamental knowledge of GenAI, including prompt engineering, to effectively select and use suitable AI learning tools, whether they are provided by the school or through commercial sources.\n\nSeveral approaches could address the tension between response accuracy and comprehensiveness identified in this study, while also following best pedagogical practices for long-term learning. For example, a hybrid system could clearly mark responses derived from the course-specific RAG database as highly reliable while flagging answers requiring external knowledge with accuracy warnings, which would allow students to assess information trustworthiness. Beyond RAG, knowledge graph architectures could enable more sophisticated cross-topic synthesis while maintaining accuracy through formal ontological constraints that explicitly map relationships between medical concepts [29](/articles/s41746-025-02022-1#ref-CR29) . Furthermore, incorporating Socratic tutoring methods, where the AI guides students through problems with targeted questions rather than direct answers, could transform the system from a passive answer service into an active learning partner that promotes deeper understanding and long-term retention [30](/articles/s41746-025-02022-1#ref-CR30) . Such systems could also adapt their approach based on context, providing direct answers during time-sensitive exam preparation while employing Socratic dialog during regular study sessions to develop critical thinking skills.\n\n## Methods\n\nThis study was conducted in two consecutive cohorts (Cohort 1: *n* =\u200992, Cohort 2: *n* =\u200998) in a second-year organ-system course focusing on Neuroscience &amp; Neurology at Geisel School of Medicine at Dartmouth that runs over 14 weeks. Classes for cohort 1 ran from October 2023 to January 2024 and for cohort 2 from October 2024 to January 2025. The course covered foundational neuroanatomy, neurophysiology, and clinical neurology topics. The instructional format was a blend of lectures, active learning sessions, case discussions, and laboratory sessions. The placement of the course was at the end of the basic science preclerkship phase and immediately before the dedicated USMLE Step 1 study phase. All course materials were made available to students via the learning management system (LMS), including lecture slide decks, assigned textbook readings, pre-class preparatory materials, a detailed syllabus, course and session learning objectives, and video recordings of all lectures.\n\n### NeuroBot TA system\n\nNeuroBot TA was developed using a commercial AI platform (getcody.ai) that supports RAG with the latest available OpenAI GPT model (GPT-4 for cohort 1 and GPT-4o for cohort 2). We assembled a comprehensive knowledge base comprising of 145 documents in the English language comprising all documents available to students through the course's learning management system, including lecture slides, excerpts from textbook chapters, prework materials, instructor handouts, and the course syllabus. To create a RAG-compliant database documents were split into smaller \"chunks\" of text (\u2009~\u2009200-300 words each) which were then converted into vector embeddings (representing the native space of LLMs) and stored in a vector database for subsequent retrieval. This enabled vector-based similarity searches, like performed by Google search, which prioritize semantic similarity. When a student asks a question in the chat interface, NeuroBot TA's backend retrieves the most relevant content chunks from the course materials in vector space based on semantic similarity to the query. The retrieved text, along with source identifiers, is then appended to the prompt given to the LLM. The system message of the Neurobot TA was set to instruct the LLM as a helpful teaching assistant and to use the retrieved course content as context to answer the question, cite the source of the information (see Fig. [1](/articles/s41746-025-02022-1#Fig1) B for a visual representation of the RAG-based approach). Importantly, NeuroBot TA was restricted to answering only questions grounded in its curated knowledge base. It also did not have access to the open internet, tools, or any data beyond the curated course documents. This was to ensure both accuracy and relevance, and to prevent the chatbot from generating any inappropriate content. The system was thus effectively an open-book exam of the course content for the AI. Questions outside the scope of the materials in the database elicited a response indicating the information was unavailable. See Supplementary Materials for screenshot examples.\n\nBefore initial deployment, pilot testing of the platform was conducted with faculty and a few volunteer students (not part of the study cohorts) to verify appropriate handling of questions, which led to prompt tuning for more refined answers. For example, we adjusted the assistant's tone to be friendly and encouraging and ensured consistent source citation to bolster trust and help students locate the referenced material for further reading.\n\n### Deployment and student access\n\nFor both cohorts, NeuroBot TA was introduced to students during the course orientation session as an optional study aid, with a live demonstration showing how to access the bot and ask questions. Access to NeuroBot TA was available 24/7 throughout the course through the LMS and via a link to the chat interface. Although no real-time moderation occurred, the course director (T.T.) had access to the anonymous chat transcripts and periodically reviewed them to monitor answer quality with a plan to correct the knowledge base or tune the prompts further should any critical inaccuracies be found. No major intervention was needed during either deployment.\n\n### Usage data collection and engagement analysis\n\nWe evaluated the TA bot's impact and student attitudes using a mixed-methods approach, collecting both quantitative usage data and qualitative feedback. Analyses were performed in Python 3.6. This quality improvement project received exemption from the Dartmouth College IRB. All data collection was anonymous, participation was voluntary, and students were informed that aggregated usage patterns might be analyzed for educational improvement purposes. Participation in the end-of-course survey was voluntary and had no effect on grades. The chat conversation logs collected over two academic years (2023-2024 &amp; 2024-2025) were analyzed for descriptive engagement metrics (e.g., conversation count and message volume). Temporal analyses compared usage during pre-exam and regular periods using paired t-tests and ANOVA as appropriate to identify students' 24/7 interaction patterns. At the end of each course, an anonymous standardized course evaluation survey was administered that included specific questions about NeuroBot TA. We asked students a yes/no question whether they have used the bot during the course and the statement \"NeuroBot TA was a helpful resource for this course\", both rated on a 5-point Likert scale. In addition, an open-ended text box invited any comments on their experience with the AI platform.\n\n### Content Analysis\n\nWe conducted systematic content analyses of both student messages sent to the bot and student feedback comments. The content analyses were done in several stages by combining GPT-4o LLM processing and human-in-the-loop verification adapted from Braun &amp; Clarke's framework [31](/articles/s41746-025-02022-1#ref-CR31) . First, conversations were preprocessed to isolate student messages from TA bot responses. Conversations with fewer than 50 characters of student input were excluded to ensure sufficient material for meaningful semantic content analysis. Initial themes were identified by randomly sampling 100 conversations/20 feedback comments and processing with GPT-4o for generation of candidate themes, which were reviewed by a human expert for coherence and relevance. Themes were then refined using a second sample ( *n* =\u200950/ *n* =\u200915), again with human verification of the refinement process to ensure accurate representation of data. Results were validated through co-occurrence analysis to establish distinctiveness of themes, resulting in a total of 8 message themes and 6 feedback themes. All valid conversations underwent systematic coding, where each was rated by GPT-4o for theme presence using a standardized scale (0\u2009=\u2009not present, 1\u2009=\u2009slightly present, 2\u2009=\u2009moderately present, 3\u2009=\u2009strongly present).\n\nThe computational coding was validated through manual review of a subset of conversations (15%). This human-LLM hybrid approach follows LLM-assisted content analysis workflows that have achieved moderate to substantial agreement with human coders [32](#ref-CR32) , [33](#ref-CR33) , [34](/articles/s41746-025-02022-1#ref-CR34) . LLM-human agreement was analyuzed with weighted ordinal Cohen's k with 95% bootstrap CIs, and, on a present/absent collapse, overall macro-F1.\n\n## Data availability\n\nAll data and analysis code are publicly available at [https://doi.org/10.6084/m9.figshare.30068977](https://doi.org/10.6084/m9.figshare.30068977) [35](/articles/s41746-025-02022-1#ref-CR35) .\n\n## References\n\n1. Pincavage, A. T. et al. A national survey of undergraduate clinical education in internal medicine. *J. Gen. Intern. Med.* **34** , 699-704 (2019). [Article](https://link.springer.com/doi/10.1007/s11606-019-04892-0) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30993614) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6502925) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20national%20survey%20of%20undergraduate%20clinical%20education%20in%20internal%20medicine&journal=J.%20Gen.%20Intern.%20Med.&doi=10.1007%2Fs11606-019-04892-0&volume=34&pages=699-704&publication_year=2019&author=Pincavage%2CAT)\n2. Pendergrast, T. R. &amp; Walter, J. M. Use of an asynchronous discussion platform during the pre-clerkship curriculum: a multiyear retrospective study. *Med. Sci. Educator* **34** , 397-403 (2024). [Article](https://link.springer.com/doi/10.1007/s40670-024-01990-5) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Use%20of%20an%20asynchronous%20discussion%20platform%20during%20the%20pre-clerkship%20curriculum%3A%20a%20multiyear%20retrospective%20study&journal=Med.%20Sci.%20Educator&doi=10.1007%2Fs40670-024-01990-5&volume=34&pages=397-403&publication_year=2024&author=Pendergrast%2CTR&author=Walter%2CJM)\n3. Ten Cate, O. &amp; Durning, S. Peer teaching in medical education: twelve reasons to move from theory to practice. *Med. Teach.* **29** , 591-599 (2007). [Article](https://doi.org/10.1080%2F01421590701606799) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17922354) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Peer%20teaching%20in%20medical%20education%3A%20twelve%20reasons%20to%20move%20from%20theory%20to%20practice&journal=Med.%20Teach.&doi=10.1080%2F01421590701606799&volume=29&pages=591-599&publication_year=2007&author=Cate%2CO&author=Durning%2CS)\n4. ten Cate, O., Gruppen, L. D., Kogan, J. R., Lingard, L. A. &amp; Teunissen, P. W. Time-variable training in medicine: theoretical considerations. *Acad. Med.* **93** , S6 (2018). [Article](https://doi.org/10.1097%2FACM.0000000000002065) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=29485480) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Time-variable%20training%20in%20medicine%3A%20theoretical%20considerations&journal=Acad.%20Med.&doi=10.1097%2FACM.0000000000002065&volume=93&publication_year=2018&author=Cate%2CO&author=Gruppen%2CLD&author=Kogan%2CJR&author=Lingard%2CLA&author=Teunissen%2CPW)\n5. Chebrolu, S. &amp; Potti, R. Impact of 'tutorial classes' on learning outcomes, among medical students: a systematic review and meta-analysis. *Al-Azhar Assiut Med. J.* **22** , 105-109 (2024). [Article](https://doi.org/10.4103%2Fazmj.azmj_43_23) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Impact%20of%20%E2%80%98tutorial%20classes%E2%80%99%20on%20learning%20outcomes%2C%20among%20medical%20students%3A%20a%20systematic%20review%20and%20meta-analysis&journal=Al-Azhar%20Assiut%20Med.%20J.&doi=10.4103%2Fazmj.azmj_43_23&volume=22&pages=105-109&publication_year=2024&author=Chebrolu%2CS&author=Potti%2CR)\n6. Ryan, L., Sheehan, K., Marion, M. I. &amp; Harbison, J. Online resources used by medical students, a literature review. *MedEdPublish* **9** , 136 (2020). [Article](https://doi.org/10.15694%2Fmep.2020.000136.1) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=38073776) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10702663) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Online%20resources%20used%20by%20medical%20students%2C%20a%20literature%20review&journal=MedEdPublish&doi=10.15694%2Fmep.2020.000136.1&volume=9&publication_year=2020&author=Ryan%2CL&author=Sheehan%2CK&author=Marion%2CMI&author=Harbison%2CJ)\n7. Zhang, P. &amp; Kamel Boulos, M. N. Generative AI in medicine and healthcare: promises, opportunities and challenges. *Future Internet* **15** , 286 (2023). [Article](https://doi.org/10.3390%2Ffi15090286) [CAS](/articles/cas-redirect/1:CAS:528:DC%2BB3sXit1Wkt7c%3D) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Generative%20AI%20in%20medicine%20and%20healthcare%3A%20promises%2C%20opportunities%20and%20challenges&journal=Future%20Internet&doi=10.3390%2Ffi15090286&volume=15&publication_year=2023&author=Zhang%2CP&author=Kamel%20Boulos%2CMN)\n8. Chokkakula, S. et al. Quantum leap in medical mentorship: exploring ChatGPT's transition from textbooks to terabytes. *Front. Med.* **12** , 1517981 (2025). [Article](https://doi.org/10.3389%2Ffmed.2025.1517981) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Quantum%20leap%20in%20medical%20mentorship%3A%20exploring%20ChatGPT%E2%80%99s%20transition%20from%20textbooks%20to%20terabytes&journal=Front.%20Med.&doi=10.3389%2Ffmed.2025.1517981&volume=12&publication_year=2025&author=Chokkakula%2CS)\n9. Ganjavi, C. et al. ChatGPT and large language models (LLMs) awareness and use. A prospective cross-sectional survey of US medical students. *PLOS Digital Health* **3** , e0000596 (2024). [Article](https://doi.org/10.1371%2Fjournal.pdig.0000596) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=39236008) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11376538) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=ChatGPT%20and%20large%20language%20models%20%28LLMs%29%20awareness%20and%20use.%20A%20prospective%20cross-sectional%20survey%20of%20US%20medical%20students&journal=PLOS%20Digital%20Health&doi=10.1371%2Fjournal.pdig.0000596&volume=3&publication_year=2024&author=Ganjavi%2CC)\n10. Rawte, V., Sheth, A. &amp; Das, A. A survey of hallucination in large foundation models. *arXiv preprint arXiv:2309.05922* (2023).\n11. Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. *Adv. Neural Inf. Process. Syst.* **33** , 9459-9474 (2020). [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Retrieval-augmented%20generation%20for%20knowledge-intensive%20nlp%20tasks&journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&volume=33&pages=9459-9474&publication_year=2020&author=Lewis%2CP)\n12. Swacha, J. &amp; Gracel, M. Retrieval-augmented generation (RAG) chatbots for education: a survey of applications. *Appl. Sci.* **15** , 4234 (2025). [Article](https://doi.org/10.3390%2Fapp15084234) [CAS](/articles/cas-redirect/1:CAS:528:DC%2BB2MXpvVWjtb4%3D) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Retrieval-augmented%20generation%20%28RAG%29%20chatbots%20for%20education%3A%20a%20survey%20of%20applications&journal=Appl.%20Sci.&doi=10.3390%2Fapp15084234&volume=15&publication_year=2025&author=Swacha%2CJ&author=Gracel%2CM)\n13. Vijayasekaran, G. et al. Personalized Learning Platform using Artificial Intelligence. in 1883-1890 (IEEE, 2024).\n14. Alrashed, F. A. et al. Incorporating technology adoption in medical education: a qualitative study of medical students' perspectives. *Adva. Med. Educ. Pract.* **15** , 615-625 (2024).\n15. Davis, E. L. Marta: a stimulant to Atlanta development? *Transp. Plan. Technol.* **10** , 241-256 (1986). [Article](https://doi.org/10.1080%2F03081068608717319) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Marta%3A%20a%20stimulant%20to%20Atlanta%20development%3F&journal=Transp.%20Plan.%20Technol.&doi=10.1080%2F03081068608717319&volume=10&pages=241-256&publication_year=1986&author=Davis%2CEL)\n16. Venkatesh, V. &amp; Davis, F. D. A theoretical extension of the technology acceptance model: four longitudinal field studies. *Manag. Sci.* **46** , 186-204 (2000). [Article](https://doi.org/10.1287%2Fmnsc.46.2.186.11926) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20theoretical%20extension%20of%20the%20technology%20acceptance%20model%3A%20four%20longitudinal%20field%20studies&journal=Manag.%20Sci.&doi=10.1287%2Fmnsc.46.2.186.11926&volume=46&pages=186-204&publication_year=2000&author=Venkatesh%2CV&author=Davis%2CFD)\n17. Scherer, R., Siddiq, F. &amp; Tondeur, J. The technology acceptance model (TAM): A meta-analytic structural equation modeling approach to explaining teachers' adoption of digital technology in education. *Comp. Educ.* **128** , 13-35 (2019). [Article](https://doi.org/10.1016%2Fj.compedu.2018.09.009) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20technology%20acceptance%20model%20%28TAM%29%3A%20A%20meta-analytic%20structural%20equation%20modeling%20approach%20to%20explaining%20teachers%E2%80%99%20adoption%20of%20digital%20technology%20in%20education&journal=Comp.%20Educ.&doi=10.1016%2Fj.compedu.2018.09.009&volume=128&pages=13-35&publication_year=2019&author=Scherer%2CR&author=Siddiq%2CF&author=Tondeur%2CJ)\n18. Leppink, J. &amp; Van den Heuvel, A. The evolution of cognitive load theory and its application to medical education. *Perspect. Med. Educ.* **4** , 119-127 (2015). [Article](https://link.springer.com/doi/10.1007/S40037-015-0192-X) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=26016429) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4456454) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20evolution%20of%20cognitive%20load%20theory%20and%20its%20application%20to%20medical%20education&journal=Perspect.%20Med.%20Educ.&doi=10.1007%2FS40037-015-0192-X&volume=4&pages=119-127&publication_year=2015&author=Leppink%2CJ&author=Heuvel%2CA)\n19. Umer, F., Naved, N., Naseem, A., Mansoor, A. &amp; Kazmi, S. M. R. Transforming education: tackling the two sigma problem with AI in journal clubs-a proof of concept. *BDJ Open* **11** , 46 (2025). [Article](https://doi.org/10.1038%2Fs41405-025-00338-4) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=40341404) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC12062218) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Transforming%20education%3A%20tackling%20the%20two%20sigma%20problem%20with%20AI%20in%20journal%20clubs%E2%80%93a%20proof%20of%20concept&journal=BDJ%20Open&doi=10.1038%2Fs41405-025-00338-4&volume=11&publication_year=2025&author=Umer%2CF&author=Naved%2CN&author=Naseem%2CA&author=Mansoor%2CA&author=Kazmi%2CSMR)\n20. Bany Abdelnabi, A. A., Soykan, B., Bhatti, D. &amp; Rabadi, G. Usefulness of large language models (LLMs) for student feedback on H&amp;P during clerkship: Artificial intelligence for personalized learning. *ACM Transactions on Computing for Healthcare* (ACM, 2025).\n21. Skryd, A. &amp; Lawrence, K. ChatGPT as a tool for medical education and clinical decision-making on the wards: case study. *JMIR Formative Res.* **8** , e51346 (2024). [Article](https://doi.org/10.2196%2F51346) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=ChatGPT%20as%20a%20tool%20for%20medical%20education%20and%20clinical%20decision-making%20on%20the%20wards%3A%20case%20study&journal=JMIR%20Formative%20Res.&doi=10.2196%2F51346&volume=8&publication_year=2024&author=Skryd%2CA&author=Lawrence%2CK)\n22. Thesen, T., Tuan, R. L., Blumer, J. &amp; Lee, M. W. LLM-based generation of USMLE-style questions with ASPET/AMSPC knowledge objectives: all RAGs and no riches. *Brit. J. Clin. Pharmacol.* (2025).\n23. Wang, A. et al. Generative AI for medical education: insights from a case study with medical students and an AI tutor for clinical reasoning. *Cuerus* **17** , 1-8 (2025).\n24. Zhang, J. S., Yoon, C., Williams, D. K. A. &amp; Pinkas, A. Exploring the usage of ChatGPT among medical students in the United States. *J. Med. Educ. Curric. Dev.* **11** , 23821205241264695 (2024). [Article](https://doi.org/10.1177%2F23821205241264695) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=39092290) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11292693) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Exploring%20the%20usage%20of%20ChatGPT%20among%20medical%20students%20in%20the%20United%20States&journal=J.%20Med.%20Educ.%20Curric.%20Dev.&doi=10.1177%2F23821205241264695&volume=11&publication_year=2024&author=Zhang%2CJS&author=Yoon%2CC&author=Williams%2CDKA&author=Pinkas%2CA)\n25. Moor, M. et al. Foundation models for generalist medical artificial intelligence. *Nature* **616** , 259-265 (2023). [Article](https://doi.org/10.1038%2Fs41586-023-05881-4) [CAS](/articles/cas-redirect/1:CAS:528:DC%2BB3sXns1yqu70%3D) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=37045921) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Foundation%20models%20for%20generalist%20medical%20artificial%20intelligence&journal=Nature&doi=10.1038%2Fs41586-023-05881-4&volume=616&pages=259-265&publication_year=2023&author=Moor%2CM)\n26. Singhal, K. et al. Toward expert-level medical question answering with large language models. *Nat. Med.* **31** , 943-950 (2025).\n27. Burk-Rafel, J. &amp; Triola, M. M. Precision medical education: institutional strategies for successful implementation. *Acad. Med.* **100** , 655-660 (2025). [Article](https://doi.org/10.1097%2FACM.0000000000005980) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=39889718) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Precision%20medical%20education%3A%20institutional%20strategies%20for%20successful%20implementation&journal=Acad.%20Med.&doi=10.1097%2FACM.0000000000005980&volume=100&pages=655-660&publication_year=2025&author=Burk-Rafel%2CJ&author=Triola%2CMM)\n28. Triola, M. M. &amp; Burk-Rafel, J. Precision medical education. *Acad. Med.* **98** , 775-781 (2023). [Article](https://doi.org/10.1097%2FACM.0000000000005227) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=37027222) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Precision%20medical%20education&journal=Acad.%20Med.&doi=10.1097%2FACM.0000000000005227&volume=98&pages=775-781&publication_year=2023&author=Triola%2CMM&author=Burk-Rafel%2CJ)\n29. Abu-Salih, B. &amp; Alotaibi, S. A systematic literature review of knowledge graph construction and application in education. *Heliyon* **10** , e25383 (2024).\n30. Bastani, H. et al. Generative AI without guardrails can harm learning: Evidence from high school mathematics. *Proc. Natl. Acad. Sci. USA* **122** , e2422633122 (2025). [Article](https://doi.org/10.1073%2Fpnas.2422633122) [CAS](/articles/cas-redirect/1:CAS:528:DC%2BB2MXitFSmtLvN) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=40560616) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Generative%20AI%20without%20guardrails%20can%20harm%20learning%3A%20Evidence%20from%20high%20school%20mathematics&journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.2422633122&volume=122&publication_year=2025&author=Bastani%2CH)\n31. Braun, V. &amp; Clarke, V. Using thematic analysis in psychology. *Qual. Res. Psychol.* **3** , 77-101 (2006). [Article](https://doi.org/10.1191%2F1478088706qp063oa) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Using%20thematic%20analysis%20in%20psychology&journal=Qual.%20Res.%20Psychol.&doi=10.1191%2F1478088706qp063oa&volume=3&pages=77-101&publication_year=2006&author=Braun%2CV&author=Clarke%2CV)\n32. Balt, E. et al. Deductively coding psychosocial autopsy interview data using a few-shot learning large language model. *Front. Public Health* **13** , 1512537 (2025). [Article](https://doi.org/10.3389%2Ffpubh.2025.1512537) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=40046117) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11879832) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Deductively%20coding%20psychosocial%20autopsy%20interview%20data%20using%20a%20few-shot%20learning%20large%20language%20model&journal=Front.%20Public%20Health&doi=10.3389%2Ffpubh.2025.1512537&volume=13&publication_year=2025&author=Balt%2CE)\n33. Zhang, H. et al. Exploring inductive and deductive qualitative coding with AI: investigating inter-rater reliability between large language model and human coders. *AHFE Open Access* **195** .\n34. Long, Y., Luo, H. &amp; Zhang, Y. Evaluating large language models in analysing classroom dialogue. *npj Sci. Learn.* **9** , 60 (2024). [Article](https://doi.org/10.1038%2Fs41539-024-00273-3) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=39358390) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11447259) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Evaluating%20large%20language%20models%20in%20analysing%20classroom%20dialogue&journal=npj%20Sci.%20Learn.&doi=10.1038%2Fs41539-024-00273-3&volume=9&publication_year=2024&author=Long%2CY&author=Luo%2CH&author=Zhang%2CY)\n35. Thesen, T. &amp; Park, S. A Generative AI Teaching Assistant for Personalized Learning in Medical Education. [https://doi.org/10.6084/m9.figshare.30068977](https://doi.org/10.6084/m9.figshare.30068977) .\n\n[Download references](https://citation-needed.springer.com/v2/references/10.1038/s41746-025-02022-1?format=refman&flavour=references)\n\n## Acknowledgements\n\nWe thank Robert A. Shumsky for inspiration and discussion.\n\n## Author information\n\n### Authors and Affiliations\n\n1. Department of Medical Education, Geisel School of Medicine at Dartmouth, Hanover, NH, USA Thomas Thesen\n2. Department of Computer Science, Dartmouth College, Hanover, NH, USA Thomas Thesen\n3. Department of Medicine, California Pacific Medical Center, Stanford School of Medicine, Palo Alto, CA, USA Soo Hwan Park\n\nAuthors\n\n1. Thomas Thesen [View author publications](/search?author=Thomas%20Thesen) Search author on: [PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Thomas%20Thesen) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Thomas%20Thesen%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n2. Soo Hwan Park [View author publications](/search?author=Soo%20Hwan%20Park) Search author on: [PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Soo%20Hwan%20Park) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Soo%20Hwan%20Park%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n\n### Contributions\n\nT.T. planned and implemented the study, analyzed the data and wrote the manuscript. S.P. contributed to data analysis and interpretation and wrote the manuscript.\n\n### Corresponding author\n\nCorrespondence to [Thomas Thesen](mailto:thomas.thesen@dartmouth.edu) .\n\n## Ethics declarations\n\n### Competing interests\n\nThe authors declare no competing interests.\n\n## Additional information\n\n**Publisher's note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\n## Supplementary information\n\n### [Supplementary information](https://static-content.springer.com/esm/art%3A10.1038%2Fs41746-025-02022-1/MediaObjects/41746_2025_2022_MOESM1_ESM.pdf)\n\n## Rights and permissions\n\n**Open Access** This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit [http://creativecommons.org/licenses/by-nc-nd/4.0/](http://creativecommons.org/licenses/by-nc-nd/4.0/) .\n\n[Reprints and permissions](https://s100.copyright.com/AppDispatchServlet?title=A%20generative%20AI%20teaching%20assistant%20for%20personalized%20learning%20in%20medical%20education&author=Thomas%20Thesen%20et%20al&contentID=10.1038%2Fs41746-025-02022-1&copyright=The%20Author%28s%29&publication=2398-6352&publicationDate=2025-11-04&publisherName=SpringerNature&orderBeanReset=true&oa=CC%20BY-NC-ND)\n\n## About this article\n\nCheck for updates. Verify currency and authenticity via CrossMark\n\n<!-- image -->\n\n### Cite this article\n\nThesen, T., Park, S.H. A generative AI teaching assistant for personalized learning in medical education. *npj Digit. Med.* **8** , 627 (2025). https://doi.org/10.1038/s41746-025-02022-1\n\n[Download citation](https://citation-needed.springer.com/v2/references/10.1038/s41746-025-02022-1?format=refman&flavour=citation)\n\n- Received : 22 July 2025\n- Accepted : 20 September 2025\n- Published : 04 November 2025\n- Version of record : 04 November 2025\n- DOI : https://doi.org/10.1038/s41746-025-02022-1\n\n## This article is cited by\n\n- [Intelligent multi-UAV path planning in complex environments: a hybrid optimization approach with direction-assisted search and adaptive refinement](https://doi.org/10.1007/s44443-025-00398-4) *Journal of King Saud University Computer and Information Sciences* (2025)\n    - Tiancheng Jin\n    - Yujie Zhu\n\n[Download PDF](/articles/s41746-025-02022-1.pdf)\n\n## Associated content\n\nCollection\n\n### [Transforming Medical Education through Artificial Intelligence](https://www.nature.com/collections/ehadbhgiji)\n\nAdvertisement\n\n## Explore content\n\n- [Research articles](/npjdigitalmed/research-articles)\n- [Reviews &amp; Analysis](/npjdigitalmed/reviews-and-analysis)\n- [News &amp; Comment](/npjdigitalmed/news-and-comment)\n- [Collections](/npjdigitalmed/collections)\n\n- [Follow us on Twitter](https://twitter.com/npjDigitalMed)\n- [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id=41746)\n- [RSS feed](https://www.nature.com/npjdigitalmed.rss)\n\n## About the journal\n\n- [Aims and scope](/npjdigitalmed/aims)\n- [Content types](/npjdigitalmed/content-types)\n- [Journal Information](/npjdigitalmed/journal-information)\n- [About the Editors](/npjdigitalmed/editors)\n- [Contact](/npjdigitalmed/contact)\n- [Editorial policies](/npjdigitalmed/editorial-policies)\n- [Calls for Papers](/npjdigitalmed/calls-for-papers)\n- [Journal Metrics](/npjdigitalmed/journal-impact)\n- [About the Partner](/npjdigitalmed/partner)\n- [Open Access](/npjdigitalmed/open-access)\n- [Early Career Researcher Editorial Fellowship](/npjdigitalmed/editorial-fellowship)\n- [Editorial Team Vacancies](/npjdigitalmed/vacancies)\n- [News and Views Student Editor](/npjdigitalmed/news-and-views-student-editor)\n- [Communication Fellowship](/npjdigitalmed/communication-fellowship)\n\n## Publish with us\n\n- [For Authors and Referees](/npjdigitalmed/for-authors-and-referees)\n- [Language editing services](https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022)\n- [Open access funding](/npjdigitalmed/open-access-funding)\n- [Submit manuscript](https://submission.springernature.com/new-submission/41746/3)\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from\n\nAll journals\n\nThis journal\n\nSearch\n\n[Advanced search](/search/advanced)\n\n### Quick links\n\n- [Explore articles by subject](/subjects)\n- [Find a job](/naturecareers)\n- [Guide to authors](/authors/index.html)\n- [Editorial policies](/authors/editorial_policies)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n- [About us](https://www.nature.com/npg_/company_info/index.html)\n- [Press releases](https://www.nature.com/npg_/press_room/press_releases.html)\n- [Press office](https://press.nature.com/)\n- [Contact us](https://support.nature.com/support/home)\n\n### Discover content\n\n- [Journals A-Z](https://www.nature.com/siteindex)\n- [Articles by subject](https://www.nature.com/subjects)\n- [protocols.io](https://www.protocols.io/)\n- [Nature Index](https://www.natureindex.com/)\n\n### Publishing policies\n\n- [Nature portfolio policies](https://www.nature.com/authors/editorial_policies)\n- [Open access](https://www.nature.com/nature-research/open-access)\n\n### Author &amp; Researcher services\n\n- [Reprints &amp; permissions](https://www.nature.com/reprints)\n- [Research data](https://www.springernature.com/gp/authors/research-data)\n- [Language editing](https://authorservices.springernature.com/language-editing/)\n- [Scientific editing](https://authorservices.springernature.com/scientific-editing/)\n- [Nature Masterclasses](https://masterclasses.nature.com/)\n- [Research Solutions](https://solutions.springernature.com/)\n\n### Libraries &amp; institutions\n\n- [Librarian service &amp; tools](https://www.springernature.com/gp/librarians/tools-services)\n- [Librarian portal](https://www.springernature.com/gp/librarians/manage-your-account/librarianportal)\n- [Open research](https://www.nature.com/openresearch/about-open-access/information-for-institutions)\n- [Recommend to library](https://www.springernature.com/gp/librarians/recommend-to-your-library)\n\n### Advertising &amp; partnerships\n\n- [Advertising](https://partnerships.nature.com/product/digital-advertising/)\n- [Partnerships &amp; Services](https://partnerships.nature.com/)\n- [Media kits](https://partnerships.nature.com/media-kits/)\n- [Branded content](https://partnerships.nature.com/product/branded-content-native-advertising/)\n\n### Professional development\n\n- [Nature Awards](https://www.nature.com/immersive/natureawards/index.html)\n- [Nature Careers](https://www.nature.com/naturecareers/)\n- [Nature Conferences](https://conferences.nature.com/)\n\n### Regional websites\n\n- [Nature Africa](https://www.nature.com/natafrica)\n- [Nature China](http://www.naturechina.com/)\n- [Nature India](https://www.nature.com/nindia)\n- [Nature Japan](https://www.natureasia.com/ja-jp)\n- [Nature Middle East](https://www.nature.com/nmiddleeast)\n\n- [Privacy Policy](https://www.nature.com/info/privacy)\n- [Use of cookies](https://www.nature.com/info/cookies)\n- Your privacy choices/Manage cookies\n- [Legal notice](https://www.nature.com/info/legal-notice)\n- [Accessibility statement](https://www.nature.com/info/accessibility-statement)\n- [Terms &amp; Conditions](https://www.nature.com/info/terms-and-conditions)\n- [Your US state privacy rights](https://www.springernature.com/ccpa)\n\nSpringer Nature\n\n<!-- image -->\n\n\u00a9 2025 Springer Nature Limited\n\nClose banner\n\nClose\n\nNature Briefing AI and Robotics\n\n<!-- image -->\n\nSign up for the *Nature Briefing: AI and Robotics* newsletter - what matters in AI and robotics research, free to your inbox weekly.\n\nEmail address\n\nSign up\n\nI agree my information will be processed in accordance with the\n\n*Nature* and Springer Nature Limited [Privacy Policy](https://www.nature.com/info/privacy) .\n\nClose banner\n\nClose\n\nGet the most important science stories of the day, free in your inbox.\n\n[Sign up for Nature Briefing: AI and Robotics](/briefing/ai-and-robotics/?brieferEntryPoint=AIAndRoboticsBriefingBanner)"
    },
    {
      "id": 2,
      "url": "https://www.nature.com/articles/s41746-025-01901-x",
      "title": "Peer perceptions of clinicians using generative AI in medical decision-making",
      "abstract": "This study investigates how a physician's use of generative AI (GenAI) in medical decision-making is perceived by peer clinicians. In a randomized experiment, 276 practicing clinicians evaluated one of three vignettes depicting a physician: (1) using no GenAI (Control), (2) using GenAI as a primary decision-making tool (GenAI-primary), and (3) using GenAI as a verification tool (GenAI-verify). Participants rated the physician depicted in the GenAI-primary condition significantly lower in clinical skill (on a 1-7 scale; mean = 3.79) than in the Control condition (5.93, *p* &lt; 0.001). Framing GenAI use as verification partially mitigated this effect (4.99, *p* &lt; 0.001). Similar patterns appeared for perceived overall healthcare experience and competence. Participants also acknowledged GenAI's value in improving accuracy (4.30, *p* &lt; 0.002) and rated institutionally customized GenAI more favorably (4.96, *p* &lt; 0.001). These findings suggest that while clinicians see GenAI as helpful, its use can negatively impact peer evaluations. These effects can be reduced, but not fully eliminated, by framing it as a verification aid.",
      "added_date": "2025-12-21T19:08:26.133578",
      "markdown": "# Peer perceptions of clinicians using generative AI in medical decision-making\n\n- [Haiyang Yang](#auth-Haiyang-Yang-Aff1-Aff2) [1](#Aff1) , [2](#Aff2) ,\n- [Tinglong Dai](#auth-Tinglong-Dai-Aff1-Aff2-Aff3-Aff4) [1](#Aff1) , [2](#Aff2) , [3](#Aff3) , [4](#Aff4) ,\n- [Nestoras Mathioudakis](#auth-Nestoras-Mathioudakis-Aff5) [5](#Aff5) ,\n- [Amy M. Knight](#auth-Amy_M_-Knight-Aff5) [5](#Aff5) ,\n- [Yuna Nakayasu](#auth-Yuna-Nakayasu-Aff1-Aff6) [1](#Aff1) , [6](#Aff6) &amp;\n- ...\n- [Risa M. Wolf](#auth-Risa_M_-Wolf-Aff1-Aff2-Aff5) [1](#Aff1) , [2](#Aff2) , [5](#Aff5)\n\nShow authors\n\n[*npj Digital Medicine*](/npjdigitalmed) **volume 8** , Article number: 530 ( 2025 ) [Cite this article](#citeas)\n\n- 14k Accesses\n- 2 Citations\n- 84 Altmetric\n- [Metrics details](/articles/s41746-025-01901-x/metrics)\n\n### Subjects\n\n- [Business](/subjects/business)\n- [Health care economics](/subjects/health-care-economics)\n- [Human behaviour](/subjects/human-behaviour)\n\n## Abstract\n\nThis study investigates how a physician's use of generative AI (GenAI) in medical decision-making is perceived by peer clinicians. In a randomized experiment, 276 practicing clinicians evaluated one of three vignettes depicting a physician: (1) using no GenAI (Control), (2) using GenAI as a primary decision-making tool (GenAI-primary), and (3) using GenAI as a verification tool (GenAI-verify). Participants rated the physician depicted in the GenAI-primary condition significantly lower in clinical skill (on a 1-7 scale; mean\u2009=\u20093.79) than in the Control condition (5.93, *p* &lt;\u20090.001). Framing GenAI use as verification partially mitigated this effect (4.99, *p* &lt;\u20090.001). Similar patterns appeared for perceived overall healthcare experience and competence. Participants also acknowledged GenAI's value in improving accuracy (4.30, *p* &lt;\u20090.002) and rated institutionally customized GenAI more favorably (4.96, *p* &lt;\u20090.001). These findings suggest that while clinicians see GenAI as helpful, its use can negatively impact peer evaluations. These effects can be reduced, but not fully eliminated, by framing it as a verification aid.\n\n### Similar content being viewed by others\n\n<!-- image -->\n\n### [A systematic review of early evidence on generative AI for drafting responses to patient messages](https://www.nature.com/articles/s44401-025-00032-5?fromPaywallRec=false)\n\nArticle\n\nOpen access\n\n23 July 2025\n\n<!-- image -->\n\n### [A framework for considering the use of generative AI for health](https://www.nature.com/articles/s41746-025-01695-y?fromPaywallRec=false)\n\nArticle\n\nOpen access\n\n21 May 2025\n\n<!-- image -->\n\n### [A systematic review and meta-analysis of diagnostic performance comparison between generative AI and physicians](https://www.nature.com/articles/s41746-025-01543-z?fromPaywallRec=false)\n\nArticle\n\nOpen access\n\n22 March 2025\n\n## Introduction\n\nThe emergence of generative artificial intelligence (GenAI) systems has generated increasing interest in their potential to enhance healthcare delivery since the introduction of ChatGPT in November 2022 [1](/articles/s41746-025-01901-x#ref-CR1) . As of early 2024, more than 70% of healthcare organizations are either pursuing or have already incorporated GenAI into their healthcare workflows [2](/articles/s41746-025-01901-x#ref-CR2) . GenAI offers significant promise in supporting physicians by streamlining clinical decision-making through the rapid analysis of patient data. While much attention has focused on using GenAI to enhance efficiency and reduce burdens associated with electronic medical records [3](#ref-CR3) , [4](#ref-CR4) , [5](/articles/s41746-025-01901-x#ref-CR5) , studies have also explored its role in *medical decision-making* , from generating differential diagnoses with clinical vignettes [6](/articles/s41746-025-01901-x#ref-CR6) to improving decision-support tools integrated into electronic medical record systems [7](/articles/s41746-025-01901-x#ref-CR7) , [8](/articles/s41746-025-01901-x#ref-CR8) .\n\nEfforts to incorporate computerized tools, including medical AI, into medical decision-making date back several decades. Early examples, such as MYCIN in the 1970s, used rule-based expert systems to recommend treatments for infectious diseases but faced challenges in usability and clinical uptake [9](/articles/s41746-025-01901-x#ref-CR9) . IBM's Watson represented a more recent attempt to augment medical decision-making through machine learning applications, particularly in oncology, but encountered mixed results due to data and implementation complexities and inconsistent clinical impact [10](/articles/s41746-025-01901-x#ref-CR10) . In contrast, GenAI marks a major shift, with its ability to process freeform, unstructured data, produce human-like responses, and provide rapid insights, offering a more flexible and accessible tool for decision support.\n\nDespite its potential, real-world applications of GenAI in medical decision-making remain limited [11](/articles/s41746-025-01901-x#ref-CR11) . One potential barrier to broader adoption is the impact of physicians' reputational concerns among their peers [12](/articles/s41746-025-01901-x#ref-CR12) . Prior studies, often in the form of laboratory experiments among trainees or non-medical practitioners, show physicians hold less favorable views of their peers who use computerized tools in patient care [13](#ref-CR13) , [14](#ref-CR14) , [15](/articles/s41746-025-01901-x#ref-CR15) . However, little is known about how *practicing* clinicians perceive peers who use decision-support tools, including GenAI, for medical decision-making. Understanding these perceptions is important because peer reputation influences professional success: In healthcare, patients frequently lack the ability to evaluate physician quality independently [16](/articles/s41746-025-01901-x#ref-CR16) , and they often rely on referrals from trusted intermediaries, such as primary care physicians, triage nurses, and specialists, to select experts [17](/articles/s41746-025-01901-x#ref-CR17) . Peer reputation not only shapes these referral networks but also plays a key role in the adoption of medical technologies [18](/articles/s41746-025-01901-x#ref-CR18) .\n\nThis study examines how a physician's use of GenAI in medical decision-making is perceived by peer clinicians, focusing on key dimensions such as competence, clinical skills, and overall healthcare experience. Using a controlled survey experiment, we evaluated these perceptions across different scenarios, including GenAI as a primary decision-making tool and GenAI framed as a verification tool. By linking these perceptions to broader issues of professional reputation and referral patterns, this study highlights the nuanced interaction between technology adoption and peer evaluations. It also sheds new light on the integration of GenAI into clinical workflows and the challenges of balancing innovation with professional trust.\n\nPrior research suggests that seeking advice or assistance can lead to perceived competence penalties, even when the advice enhances decision quality; these effects appear to be attenuated when the advice-seeker signals humility or deference to social norms [19](/articles/s41746-025-01901-x#ref-CR19) . In clinical contexts, physicians' use of GenAI may similarly be interpreted as a signal of lower personal competence. This possibility aligns with a broader body of advice-taking literature, which shows that reliance on external input can be perceived as a weakness rather than a strength [20](/articles/s41746-025-01901-x#ref-CR20) .\n\nBased on these insights, we formulated two key hypotheses. First, we hypothesized that, compared with those who do not use GenAI, physicians who use GenAI as a primary decision-making tool would be perceived as having lower clinical skills, providing a worse overall healthcare experience, and being less competent overall. Second, we hypothesized that presenting GenAI as a verification tool, rather than as a primary decision-making tool, would partially-but not fully-mitigate these negative perceptions.\n\n## Results\n\nA total of 276 clinicians participated in the study, including 178 physicians, 28 fellows/residents, 60 advanced practice providers (physician assistants and nurse practitioners), and 10 individuals in other clinical roles. An additional 123 individuals started the survey but did not complete it and thus were not included in the analysis. In the total cohort, most participants were aged 35-54 years; 60.1% were female, 19.2% Asian, 4.7% Black, and 62.3% White. As shown in Table [1](/articles/s41746-025-01901-x#Tab1) , participants were balanced across years of practice experience and practice setting (inpatient and outpatient). Baseline demographic and workforce characteristics did not differ significantly across the three conditions. For clarity, the \"GenAI-primary\" condition refers to a physician using GenAI as the primary decision-making aid, whereas in the \"GenAI-verify\" condition, the physician uses GenAI only to verify their decision. A summary of participants' responses is provided in Table [2](/articles/s41746-025-01901-x#Tab2) .\n\n### Clinical Skills\n\nRatings of clinical skills differed significantly across the three conditions (F(2, 273)\u2009=\u200945.45, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.25; Fig. [1](/articles/s41746-025-01901-x#Fig1) , first panel **)** . The mean (SD) clinical skills score for the Control condition was 5.93 (1.24), for GenAI-primary was 3.79 (1.62), and for GenAI-verify was 4.99 (1.67). The difference between the GenAI-primary and Control conditions was statistically significant (F(1, 273)\u2009=\u200990.30, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.25), as was the difference between GenAI-verify and Control conditions (F(1, 273)\u2009=\u200917.33, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.06). Presenting GenAI as a verification tool partially mitigated this effect, though the clinical skills rating remained lower than in the Control condition (F(1, 273)\u2009=\u200928.99, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.10).\n\nFig. 1: Clinicians' evaluations of clinical skill, overall healthcare experience, and overall competence across conditions.\n\n<!-- image -->\n\n### Overall Healthcare Experience\n\nEvaluations of overall healthcare experience differed significantly across the three conditions (F(2, 273)\u2009=\u200934.38, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.20; Fig. [1](/articles/s41746-025-01901-x#Fig1) , second panel). The mean (SD) evaluations were 4.48 (0.82) in the Control condition, 3.08 (1.30) in the GenAI-primary condition, and 3.72 (1.24) in the GenAI-verify condition. Compared with those in the Control condition, evaluations in the GenAI-primary condition (F(1, 273)\u2009=\u200968.67, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.20) and GenAI-verify condition (F(1, 273)\u2009=\u200920.02, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.07) were significantly lower. The healthcare experience was rated significantly lower in the GenAI-primary condition than in the GenAI-verify condition (F(1, 273)\u2009=\u200914.77, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.05). That is, while presenting GenAI as a verification tool improved healthcare experience evaluations, they remained lower than those in the Control condition.\n\nMediation analysis revealed that clinical skills ratings mediated the relationship between study conditions and healthcare experience evaluations. This analysis showed the relative indirect effect of D GenAI-primary through clinical skill ratings was significant (\u03b2\u2009=\u2009\u22121.30, SE\u2009=\u20090.15, 95% CI: [\u22121.59, \u22121.01]), and the relative indirect effect of D GenAI-verify was also significant (\u03b2\u2009=\u2009\u22120.57, SE\u2009=\u20090.13, 95% CI: [\u22120.83, \u22120.31]). In other words, generative AI usage reduced the ratings of the physician's clinical skills, which in turn negatively impacted the evaluations of the overall healthcare experience provided by the physician.\n\n### Overall Competence\n\nOverall competence evaluations differed significantly across the three conditions (F(2, 273)\u2009=\u200949.60, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.27; Fig. [1](/articles/s41746-025-01901-x#Fig1) , third panel). The mean (SD) ratings were 5.99 (1.25) in the Control condition, 3.71 (1.61) in the GenAI-primary condition, and 4.94 (1.74) in the GenAI-verify condition. Compared with those in the Control condition, competence evaluations were significantly lower in the GenAI-primary condition (F(1, 273)\u2009=\u200998.91, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.27) and GenAI-verify condition (F(1, 273)\u2009=\u200921.13, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.07) conditions. Competence evaluations in the GenAI-primary condition were significantly lower than those in the GenAI-verify condition (F(1, 273)\u2009=\u200929.09, *p* &lt;\u20090.001, \u03b7 p \u00b2\u2009=\u20090.10). That is, presenting GenAI as a verification tool improved competence evaluations, but they remained significantly lower than in the Control condition.\n\nMediation analysis revealed that clinical skills ratings mediated the relationship between study conditions and competence evaluations. The relative indirect effect of D GenAI-primary through clinical skill ratings was significant (\u03b2\u2009=\u2009\u22121.93, SE\u2009=\u20090.20, 95% CI: [\u22122.33, \u22121.55]), and the relative indirect effect of D GenAI-verify was also significant (\u03b2\u2009=\u2009\u22120.85, SE\u2009=\u20090.20, 95% CI: [\u22121.24, \u22120.46]). The use of GenAI decreased ratings of the physician's clinical skills, which in turn led to lower competence evaluations.\n\n### Perceived Usefulness of GenAI\n\nThe perceived usefulness of GenAI technologies did not differ across the three conditions. Participants rated GenAI technologies as useful for ensuring clinical assessment accuracy (mean [SD], 4.30 [1.65]; *t* =\u20093.06, *p* &lt;\u20090.002, Cohen's d\u2009=\u20090.18), and they rated customized GenAI as even more useful (mean [SD], 4.96 [1.65]; *t* =\u20099.64, *p* &lt;\u20090.001, Cohen's d\u2009=\u20090.58). That is, participants perceived GenAI as a useful tool for clinical assessment.\n\n## Discussion\n\nIn a study of 276 clinicians at a major hospital system, we found that while clinicians acknowledge the potential of GenAI to enhance medical decision-making, they consistently rate physicians using such tools as being less clinically skilled, less competent, and delivering a lower quality healthcare experience. Although framing GenAI usage as a verification tool reduces some of these negative perceptions, it does not fully mitigate them. These findings carry significant implications for the development and deployment of AI tools in medicine, particularly as they relate to physician perceptions and patient care experience.\n\nOur findings align with advice-taking theory [19](/articles/s41746-025-01901-x#ref-CR19) , [20](/articles/s41746-025-01901-x#ref-CR20) , suggesting that reliance on external input, such as GenAI, can trigger penalties on perceived competence. Observers may apply attributional discounting, attributing the physician's success more to the AI tool and less to the physician's actual abilities [21](/articles/s41746-025-01901-x#ref-CR21) . From a reputation signaling perspective, visible utilization of GenAI may undermine a physician's perceived clinical expertise among peers [22](/articles/s41746-025-01901-x#ref-CR22) . These dynamics may explain why even framing GenAI use as verification did not fully restore peer evaluations to baseline levels.\n\nTo our knowledge, this is the first study to examine clinicians' perceptions of medical decision-making and healthcare experience in the context of GenAI. A strength of this study was the use of a between-participants design with three clinical scenarios to evaluate whether the use of GenAI for medical decision-making influences clinicians' perceptions of care quality. It has been said that clinicians who use AI will replace those who do not use AI [22](/articles/s41746-025-01901-x#ref-CR22) ; it is thus important to understand the implications of clinical AI utilization. General use of medical decision-support tools augments the expertise of clinicians, and can lead to greater diagnostic accuracy, potentially improving the capabilities of clinicians and enhancing their ability to provide evidence-based care [23](/articles/s41746-025-01901-x#ref-CR23) . Historically, the perception of clinical expertise was characterized by competence in clinical skills, advanced clinical judgment, cognitive abilities, a deep understanding of clinical reasoning and diagnostic processes, and scholarship [24](/articles/s41746-025-01901-x#ref-CR24) , [25](/articles/s41746-025-01901-x#ref-CR25) . This is aligned with our findings that clinicians perceived the physician who did not use GenAI to have superior clinical skills, whereas physicians who relied on GenAI for medical decision-making were perceived as less competent in their clinical skills. Given the increasing use of AI in medicine, this perception is likely to change in the coming years, but not without challenges along the way.\n\nWhile there are benefits to computerized and GenAI-based decision support systems, successful implementation of these tools will require overcoming clinician and institutional resistance, and modifying perceptions of clinical expertise with use of these systems. The practicing clinician still needs to leverage their clinical acumen within the context of the specific clinical situation, but the decision-making process can be further refined and supported through the integration of GenAI clinician-decision support tools that enhance precision and efficiency. As GenAI reshapes medical decision support, clinicians must remain open to adopting innovative systems that are rigorously validated for efficacy and safety, ensuring they complement clinical expertise while improving patient care outcomes.\n\nIn addition, using GenAI as the initial decision-maker can introduce confirmation bias, a cognitive tendency to favor information that confirms one's initial hypothesis [26](/articles/s41746-025-01901-x#ref-CR26) , [27](/articles/s41746-025-01901-x#ref-CR27) . In practice, relying on GenAI first may make it cognitively harder for clinicians to consider alternative diagnoses or treatments, potentially leading to overreliance. By contrast, using GenAI as a verification tool-after formulating one's own plan-may help mitigate this bias. This important consideration further differentiates the two GenAI use cases.\n\nThis study has several limitations. First, while the respondents included a variety of clinicians, the majority were physicians, and thus results may not be generalizable to all clinicians within a hospital system. Second, the diabetic care scenarios used in the study were developed and prescreened by clinicians to ensure both realism and readability. The amount of time participants spent responding to their assigned healthcare scenario did not differ across the three conditions ( *p* &gt;\u20090.30), indicating that the effects and mediational patterns observed in the study could not simply be attributed to differences in effort or comprehension. Future research utilizing different scenarios and different measures can further examine the generalizability of our findings. Third, given the population we examined (i.e., frontline clinicians), we did not have an opportunity to perform a separate formal instrument development or validation for our survey constructs prior to data collection. Our measures (e.g., clinical skills, healthcare experience) were chosen based on literature and expert input, and we treated the Likert-scale responses as approximately continuous for analysis, an approach not uncommon in survey experiments [28](/articles/s41746-025-01901-x#ref-CR28) , [29](/articles/s41746-025-01901-x#ref-CR29) , but these choices may introduce measurement limitations. Future studies should employ rigorous scale development and validation to refine the measurement approach. Fourth, this study utilized a convenience sample, yet the research still offers novel data on the perspective of clinicians on use of GenAI for medical decision-making. Fifth, this study was conducted at one health system, and findings at other health systems in other parts of the country or the world may be different. Finally, our study focused exclusively on peer perceptions among clinicians. Future studies should examine how patients perceive clinicians' use of GenAI technologies, and how these perceptions influence trust, adherence, and clinical outcomes. Patients' trust and acceptance of clinicians' recommendations may be influenced differently by GenAI usage [30](/articles/s41746-025-01901-x#ref-CR30) , with important implications for adherence and outcomes.\n\nIn conclusion, this study suggests that although clinicians clearly perceive GenAI technologies as beneficial for optimal medical decision-making, they evaluate their peers who utilize such technologies as lower in clinical skills, less effective in providing an optimal healthcare experience, and less competent. Presenting GenAI technologies as tools for verification purposes can help reduce these negative evaluations, but does not eliminate them. Although these findings highlight that there may be challenges with the adoption and increasing use of GenAI in medicine, they also emphasize the importance of thoughtful approaches to its implementation in healthcare settings.\n\n## Methods\n\nJohns Hopkins Medicine is a six-entity health system with four hospitals in Maryland, one in Washington, D.C., and one in Florida. Clinicians at these hospitals include attending physicians, residents, fellows, and advanced practice providers (APPs), such as physician assistants and nurse practitioners. The survey invitation was distributed via a departmental listserv that included approximately 4000 clinicians across various specialties, encompassing physicians, APPs, and other clinical staff. Clinicians at the two academic hospitals in Baltimore, MD, were invited to participate in the study, which was conducted using the Qualtrics survey tool. We conservatively assumed that the effect sizes would be relatively moderate. Using G*Power Version 3.1.9.7 ( *f* =\u20090.2, power\u2009=\u20090.8, *\u03b1* =\u20090.05, omnibus one-way ANOVA with 3 conditions), we estimated the minimum sample size per condition to be about 82 participants per condition (i.e., a minimum sample size of 246 participants in total). The survey link was sent weekly for three consecutive weeks in August 2024. To ensure privacy, IP and other personally identifiable information were not tracked in the study. The survey was open between August 1, 2024, and September 10, 2024, with participation being voluntary and based on availability and willingness, thereby forming a convenience sample. There were no restrictions on inclusion based on specialty or department, and no incentives were offered for survey completion. The study protocol was approved by the Johns Hopkins School of Medicine Institutional Review Board (JHM IRB00393208). Participants provided informed consent by proceeding past the introductory survey page, which described the study's purpose, procedures, and their rights as participants, per the IRB-approved protocol.\n\nParticipants were randomly assigned to one of three conditions: **Control** (no GenAI involved), **GenAI-primary** (physician using GenAI for medical decision-making), or **GenAI-verify** (physician using GenAI to verify medical decision-making). In the GenAI-primary and GenAI-verify conditions, GenAI was explicitly referenced. In the Control condition, participants were presented with a clinical scenario in which a physician assesses a patient with diabetes and recommends a new antihyperglycemic medication *without* any mention or use of GenAI. In the GenAI-primary condition, the same scenario and recommendation were presented, but the physician was noted to have used GenAI during the decision-making process. In the GenAI-verify condition, the scenario and recommendation were identical to the GenAI-primary scenario, but the use of GenAI was framed as \"an additional level of verification.\" The clinical scenarios (see Supplementary Note [1](/articles/s41746-025-01901-x#MOESM1) ) were developed collaboratively with practicing physicians and pretested to ensure realism, clarity, and relevance to current clinical practice. The scenarios were iteratively refined with feedback from study collaborators and then independently reviewed by an internist, a medical student, and a surgeon, all of whom confirmed that the vignettes were clear and clinically realistic. The case involved adult diabetes management, a common and routine decision-making context selected to enhance ecological validity.\n\nAll participants completed the same set of measures. First, they rated the physician's clinical skills using two Likert scale items: clinical management skills (1\u2009=\u2009poor, 7\u2009=\u2009excellent) and the appropriateness of the recommendation to start a new medication (1\u2009=\u2009poor, 7\u2009=\u2009excellent). This type of Likert scale approach has been used to evaluate physician performance and satisfaction in both clinical and training contexts [31](/articles/s41746-025-01901-x#ref-CR31) . Participants then provided two overall evaluations: the quality of the healthcare experience delivered by the physician (rated from 1 to 5 stars) and the physician's competence as a medical doctor (1\u2009=\u2009not competent at all, 7\u2009=\u2009highly competent). The 5-star scale approach is not only commonly used in healthcare evaluations and satisfaction research [32](/articles/s41746-025-01901-x#ref-CR32) , [33](/articles/s41746-025-01901-x#ref-CR33) , but also widely used by hospitals and payers for public quality reporting and patient-experience benchmarking [34](/articles/s41746-025-01901-x#ref-CR34) , [35](/articles/s41746-025-01901-x#ref-CR35) . Participants also rated their perceptions of GenAI technologies on two items: the extent to which GenAI could help ensure the accuracy of the physician's clinical assessment (1\u2009=\u2009not at all, 7\u2009=\u2009very much) and the extent to which GenAI customized for Johns Hopkins could enhance assessment accuracy (1\u2009=\u2009not at all, 7\u2009=\u2009very much). In addition, participants provided basic demographic information, including age and gender. Responses to the two clinical skill items were averaged to create a composite measure of clinical skills ( *r* =\u20090.86, *p* &lt;\u20090.001) for subsequent analyses.\n\n### Statistical analysis\n\nSummary statistics were calculated to describe participants' demographic characteristics. We used analysis of variance (ANOVA) to compare responses across all three experimental conditions (Control, GenAI-primary, GenAI-verify), treating the 7-point Likert responses as approximately continuous measures for these analyses (as in similar survey experiments [28](/articles/s41746-025-01901-x#ref-CR28) , [29](/articles/s41746-025-01901-x#ref-CR29) ), with post-hoc contrast analyses to assess pairwise differences. Tukey HSD and Bonferroni tests were conducted, which confirmed that all significant contrast results remained robust after corrections for multiple comparisons. Two-tailed t-tests were employed to assess deviations from indifference points.\n\nTo examine whether clinical skill ratings mediated the relationship between study conditions and the core dependent measures (healthcare experience evaluations and competence evaluations), two dummy variables were created: one for the GenAI-primary condition (D GenAI-primary ) and one for the GenAI-verify condition (D GenAI-verify ). A value of zero on both dummy variables represented the Control condition, which served as the reference group for comparisons. Two multicategorical mediation analyses were conducted (PROCESS Model 4; 5000 bootstrap resamples [36](/articles/s41746-025-01901-x#ref-CR36) ) with D GenAI-primary and D GenAI-verify as the independent variables. SPSS Version 28 was used in these analyses. The figure was created using Python 3.11.\n\n## Data availability\n\nThe datasets generated and analyzed during this study are available from the corresponding author upon reasonable request.\n\n## Code availability\n\nNo custom code was used in this study.\n\n## References\n\n1. Minssen, T., Vayena, E. &amp; Cohen, I. G. The challenges for regulating medical use of ChatGPT and other large language models. *JAMA* **330** , 315 (2023). [Article](https://doi.org/10.1001%2Fjama.2023.9651) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=37410482) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20challenges%20for%20regulating%20medical%20use%20of%20ChatGPT%20and%20other%20large%20language%20models&journal=JAMA&doi=10.1001%2Fjama.2023.9651&volume=330&publication_year=2023&author=Minssen%2CT&author=Vayena%2CE&author=Cohen%2CIG)\n2. Lamb, J., Israelstam, G., Agarwal, R. &amp; Bhasker, S. *The Future of Generative AI in Healthcare* . [https://www.mckinsey.com/industries/healthcare/our-insights/generative-ai-in-healthcare-adoption-trends-and-whats-next](https://www.mckinsey.com/industries/healthcare/our-insights/generative-ai-in-healthcare-adoption-trends-and-whats-next) (2024).\n3. Ayers, J. W. et al. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. *JAMA Intern Med* **183** , 589 (2023). [Article](https://doi.org/10.1001%2Fjamainternmed.2023.1838) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=37115527) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10148230) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Comparing%20physician%20and%20artificial%20intelligence%20chatbot%20responses%20to%20patient%20questions%20posted%20to%20a%20public%20social%20media%20forum&journal=JAMA%20Intern%20Med&doi=10.1001%2Fjamainternmed.2023.1838&volume=183&publication_year=2023&author=Ayers%2CJW)\n4. Tierney, A. A. et al. Ambient artificial intelligence scribes to alleviate the burden of clinical documentation. *NEJM Catalyst* **5** (2024).\n5. Tai-Seale, M. et al. AI-generated draft replies integrated into health records and physicians' electronic communication. *JAMA Netw. Open* **7** , e246565 (2024). [Article](https://doi.org/10.1001%2Fjamanetworkopen.2024.6565) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=38619840) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11019394) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=AI-generated%20draft%20replies%20integrated%20into%20health%20records%20and%20physicians%E2%80%99%20electronic%20communication&journal=JAMA%20Netw.%20Open&doi=10.1001%2Fjamanetworkopen.2024.6565&volume=7&publication_year=2024&author=Tai-Seale%2CM)\n6. Goh, E. et al. Large language model influence on diagnostic reasoning: a randomized clinical trial. *JAMA Netw. Open* **7** , e2440969 (2024). [Article](https://doi.org/10.1001%2Fjamanetworkopen.2024.40969) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=39466245) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11519755) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Large%20language%20model%20influence%20on%20diagnostic%20reasoning%3A%20a%20randomized%20clinical%20trial&journal=JAMA%20Netw.%20Open&doi=10.1001%2Fjamanetworkopen.2024.40969&volume=7&publication_year=2024&author=Goh%2CE)\n7. Liu, S. et al. Using AI-generated suggestions from ChatGPT to optimize clinical decision support. *J. Am. Med. Inform. Assoc.* **30** , 1237-1245 (2023). [Article](https://doi.org/10.1093%2Fjamia%2Focad072) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=37087108) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10280357) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Using%20AI-generated%20suggestions%20from%20ChatGPT%20to%20optimize%20clinical%20decision%20support&journal=J.%20Am.%20Med.%20Inform.%20Assoc.&doi=10.1093%2Fjamia%2Focad072&volume=30&pages=1237-1245&publication_year=2023&author=Liu%2CS)\n8. Rodriguez, D. V. et al. Leveraging generative AI tools to support the development of digital solutions in health care research: case study. *JMIR Hum. Factors* **11** , e52885 (2024). [Article](https://doi.org/10.2196%2F52885) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=38446539) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10955400) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Leveraging%20generative%20AI%20tools%20to%20support%20the%20development%20of%20digital%20solutions%20in%20health%20care%20research%3A%20case%20study&journal=JMIR%20Hum.%20Factors&doi=10.2196%2F52885&volume=11&publication_year=2024&author=Rodriguez%2CDV)\n9. Dai, T. &amp; Abr\u00e0moff, M. D. Incorporating Artificial Intelligence into Healthcare Workflows: Models and Insights. in *Tutorials in Operations Research: Advancing the Frontiers of OR/MS: From Methodologies to Applications* 133-155. [https://doi.org/10.1287/educ.2023.0257](https://doi.org/10.1287/educ.2023.0257) (INFORMS, 2023).\n10. Strickland, E. IBM Watson, heal thyself: How IBM overpromised and underdelivered on AI health care. *IEEE Spectr.* **56** , 24-31 (2019). [Article](https://doi.org/10.1109%2FMSPEC.2019.8678513) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=IBM%20Watson%2C%20heal%20thyself%3A%20How%20IBM%20overpromised%20and%20underdelivered%20on%20AI%20health%20care&journal=IEEE%20Spectr.&doi=10.1109%2FMSPEC.2019.8678513&volume=56&pages=24-31&publication_year=2019&author=Strickland%2CE)\n11. Yim, D., Khuntia, J., Parameswaran, V. &amp; Meyers, A. Preliminary evidence of the use of generative AI in health care clinical services: systematic narrative review. *JMIR Med Inf.* **12** , e52073 (2024). [Article](https://doi.org/10.2196%2F52073) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Preliminary%20evidence%20of%20the%20use%20of%20generative%20AI%20in%20health%20care%20clinical%20services%3A%20systematic%20narrative%20review&journal=JMIR%20Med%20Inf.&doi=10.2196%2F52073&volume=12&publication_year=2024&author=Yim%2CD&author=Khuntia%2CJ&author=Parameswaran%2CV&author=Meyers%2CA)\n12. Dai, T. &amp; Tayur, S. Designing AI-augmented healthcare delivery systems for physician buy-in and patient acceptance. *Prod. Oper. Manag.* **31** , 4443-4451 (2022). [Article](https://doi.org/10.1111%2Fpoms.13850) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Designing%20AI-augmented%20healthcare%20delivery%20systems%20for%20physician%20buy-in%20and%20patient%20acceptance&journal=Prod.%20Oper.%20Manag.&doi=10.1111%2Fpoms.13850&volume=31&pages=4443-4451&publication_year=2022&author=Dai%2CT&author=Tayur%2CS)\n13. Arkes, H. R., Shaffer, V. A. &amp; Medow, M. A. Patients derogate physicians who use a computer-assisted diagnostic aid. *Med Decis. Mak.* **27** , 189-202 (2007). [Article](https://doi.org/10.1177%2F0272989X06297391) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Patients%20derogate%20physicians%20who%20use%20a%20computer-assisted%20diagnostic%20aid&journal=Med%20Decis.%20Mak.&doi=10.1177%2F0272989X06297391&volume=27&pages=189-202&publication_year=2007&author=Arkes%2CHR&author=Shaffer%2CVA&author=Medow%2CMA)\n14. Wolf, J. R. Do IT students prefer doctors who use IT?. *Comput. Hum. Behav.* **35** , 287-294 (2014). [Article](https://doi.org/10.1016%2Fj.chb.2014.03.020) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Do%20IT%20students%20prefer%20doctors%20who%20use%20IT%3F&journal=Comput.%20Hum.%20Behav.&doi=10.1016%2Fj.chb.2014.03.020&volume=35&pages=287-294&publication_year=2014&author=Wolf%2CJR)\n15. Yin, Y., Jia, N. &amp; Wakslak, C. J. AI can help people feel heard, but an AI label diminishes this impact. *Proc. Natl Acad. Sci. USA.* **121** , e2319112121 (2024). [Article](https://doi.org/10.1073%2Fpnas.2319112121) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=38551835) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10998586) [CAS](/articles/cas-redirect/1:CAS:528:DC%2BB2cXps1elsLc%3D) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=AI%20can%20help%20people%20feel%20heard%2C%20but%20an%20AI%20label%20diminishes%20this%20impact&journal=Proc.%20Natl%20Acad.%20Sci.%20USA.&doi=10.1073%2Fpnas.2319112121&volume=121&publication_year=2024&author=Yin%2CY&author=Jia%2CN&author=Wakslak%2CCJ)\n16. Makary, M. *Unaccountable: What Hospitals Won't Tell You and How Transparency Can Revolutionize Health Care* . (Bloomsbury Publishing USA, New York, 2013).\n17. Harris, K. M. How do patients choose physicians? Evidence from a national survey of enrollees in employment-related health plans. *Health Serv. Res.* **38** , 711-732 (2003). [Article](https://doi.org/10.1111%2F1475-6773.00141) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=12785569) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1360911) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=How%20do%20patients%20choose%20physicians%3F%20Evidence%20from%20a%20national%20survey%20of%20enrollees%20in%20employment-related%20health%20plans&journal=Health%20Serv.%20Res.&doi=10.1111%2F1475-6773.00141&volume=38&pages=711-732&publication_year=2003&author=Harris%2CKM)\n18. Navathe, A. &amp; David, G. The formation of peer reputation among physicians and its effect on technology adoption. *J. Hum. Cap.* **3** , 289-322 (2009). [Article](https://doi.org/10.1086%2F652900) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20formation%20of%20peer%20reputation%20among%20physicians%20and%20its%20effect%20on%20technology%20adoption&journal=J.%20Hum.%20Cap.&doi=10.1086%2F652900&volume=3&pages=289-322&publication_year=2009&author=Navathe%2CA&author=David%2CG)\n19. Cojuharenco, I. &amp; Karelaia, N. When leaders ask questions: Can humility premiums buffer the effects of competence penalties?. *Organ. Behav. Hum. Decis. Process.* **156** , 113-134 (2020). [Article](https://doi.org/10.1016%2Fj.obhdp.2019.12.001) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=When%20leaders%20ask%20questions%3A%20Can%20humility%20premiums%20buffer%20the%20effects%20of%20competence%20penalties%3F&journal=Organ.%20Behav.%20Hum.%20Decis.%20Process.&doi=10.1016%2Fj.obhdp.2019.12.001&volume=156&pages=113-134&publication_year=2020&author=Cojuharenco%2CI&author=Karelaia%2CN)\n20. K\u00e4mmer, J. E., Choshen-Hillel, S., M\u00fcller-Trede, J., Black, S. L. &amp; Weibler, J. A systematic review of empirical studies on advice-based decisions in behavioral and organizational research. *Decision* **10** , 107-137 (2023). [Article](https://doi.org/10.1037%2Fdec0000199) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20systematic%20review%20of%20empirical%20studies%20on%20advice-based%20decisions%20in%20behavioral%20and%20organizational%20research&journal=Decision&doi=10.1037%2Fdec0000199&volume=10&pages=107-137&publication_year=2023&author=K%C3%A4mmer%2CJE&author=Choshen-Hillel%2CS&author=M%C3%BCller-Trede%2CJ&author=Black%2CSL&author=Weibler%2CJ)\n21. Weiner, B. An attributional theory of achievement motivation and emotion. *Psychol. Rev.* **92** , 548-573 (1985). [Article](https://doi.org/10.1037%2F0033-295X.92.4.548) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=3903815) [CAS](/articles/cas-redirect/1:STN:280:DyaL28%2FktlWrtA%3D%3D) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=An%20attributional%20theory%20of%20achievement%20motivation%20and%20emotion&journal=Psychol.%20Rev.&doi=10.1037%2F0033-295X.92.4.548&volume=92&pages=548-573&publication_year=1985&author=Weiner%2CB)\n22. Dai, T. &amp; Singh, S. Artificial intelligence on call: the physician's decision of whether to use AI in clinical practice. *J. Mark. Res.* (forthcoming) [https://doi.org/10.1177/00222437251332898](https://doi.org/10.1177/00222437251332898) (2025).\n23. Topol, E. J. High-performance medicine: the convergence of human and artificial intelligence. *Nat. Med* **25** , 44-56 (2019). [Article](https://doi.org/10.1038%2Fs41591-018-0300-7) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30617339) [CAS](/articles/cas-redirect/1:CAS:528:DC%2BC1MXmvVOgsbs%3D) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=High-performance%20medicine%3A%20the%20convergence%20of%20human%20and%20artificial%20intelligence&journal=Nat.%20Med&doi=10.1038%2Fs41591-018-0300-7&volume=25&pages=44-56&publication_year=2019&author=Topol%2CEJ)\n24. Dai, T. &amp; Singh, S. Conspicuous by its absence: diagnostic expert testing under uncertainty. *Mark. Sci.* **39** , 540-563 (2020). [Article](https://doi.org/10.1287%2Fmksc.2019.1201) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Conspicuous%20by%20its%20absence%3A%20diagnostic%20expert%20testing%20under%20uncertainty&journal=Mark.%20Sci.&doi=10.1287%2Fmksc.2019.1201&volume=39&pages=540-563&publication_year=2020&author=Dai%2CT&author=Singh%2CS)\n25. Rosenbaum, L. The less-is-more crusade - are we overmedicalizing or oversimplifying?. *N. Engl. J. Med* **377** , 2392-2397 (2017). [Article](https://doi.org/10.1056%2FNEJMms1713248) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=29236644) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20less-is-more%20crusade%20%E2%80%94%20are%20we%20overmedicalizing%20or%20oversimplifying%3F&journal=N.%20Engl.%20J.%20Med&doi=10.1056%2FNEJMms1713248&volume=377&pages=2392-2397&publication_year=2017&author=Rosenbaum%2CL)\n26. Nickerson, R. S. Confirmation bias: a ubiquitous phenomenon in many guises. *Rev. Gen. Psychol.* **2** , 175-220 (1998). [Article](https://doi.org/10.1037%2F1089-2680.2.2.175) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Confirmation%20bias%3A%20a%20ubiquitous%20phenomenon%20in%20many%20guises&journal=Rev.%20Gen.%20Psychol.&doi=10.1037%2F1089-2680.2.2.175&volume=2&pages=175-220&publication_year=1998&author=Nickerson%2CRS)\n27. Tversky, A. &amp; Kahneman, D. Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty. *Science* **185** , 1124-1131 (1974). [Article](https://doi.org/10.1126%2Fscience.185.4157.1124) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17835457) [CAS](/articles/cas-redirect/1:STN:280:DC%2BC3cvls12ruw%3D%3D) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Judgment%20under%20Uncertainty%3A%20Heuristics%20and%20Biases%3A%20Biases%20in%20judgments%20reveal%20some%20heuristics%20of%20thinking%20under%20uncertainty&journal=Science&doi=10.1126%2Fscience.185.4157.1124&volume=185&pages=1124-1131&publication_year=1974&author=Tversky%2CA&author=Kahneman%2CD)\n28. Norman, G. Likert scales, levels of measurement and the \"laws\" of statistics. *Adv. Health Sci. Educ.* **15** , 625-632 (2010). [Article](https://link.springer.com/doi/10.1007/s10459-010-9222-y) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Likert%20scales%2C%20levels%20of%20measurement%20and%20the%20%E2%80%9Claws%E2%80%9D%20of%20statistics&journal=Adv.%20Health%20Sci.%20Educ.&doi=10.1007%2Fs10459-010-9222-y&volume=15&pages=625-632&publication_year=2010&author=Norman%2CG)\n29. Herrmann, A. et al. Acceptability of health-only versus climate-and-health framings in lifestyle-related climate-sensitive health counselling: results of a randomised survey experiment in Germany. *Lancet Planet. Health* **9** , e456-e466 (2025). [Article](https://doi.org/10.1016%2FS2542-5196%2825%2900110-X) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=40516537) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Acceptability%20of%20health-only%20versus%20climate-and-health%20framings%20in%20lifestyle-related%20climate-sensitive%20health%20counselling%3A%20results%20of%20a%20randomised%20survey%20experiment%20in%20Germany&journal=Lancet%20Planet.%20Health&doi=10.1016%2FS2542-5196%2825%2900110-X&volume=9&pages=e456-e466&publication_year=2025&author=Herrmann%2CA)\n30. Sagona, M., Dai, T., Macis, M. &amp; Darden, M. Trust in AI-assisted health systems and AI's trust in humans. *npj Health Syst.* **2** , 10 (2025). [Article](https://doi.org/10.1038%2Fs44401-025-00016-5) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Trust%20in%20AI-assisted%20health%20systems%20and%20AI%E2%80%99s%20trust%20in%20humans&journal=npj%20Health%20Syst.&doi=10.1038%2Fs44401-025-00016-5&volume=2&publication_year=2025&author=Sagona%2CM&author=Dai%2CT&author=Macis%2CM&author=Darden%2CM)\n31. Daugherty, S. R., Baldwin, D. C. Jr &amp; Rowley, B. D. Learning, satisfaction, and mistreatment during medical internship: a national survey of working conditions. *JAMA* **279** , 1194 (1998). [Article](https://doi.org/10.1001%2Fjama.279.15.1194) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=9555759) [CAS](/articles/cas-redirect/1:STN:280:DyaK1c3hsFSnsA%3D%3D) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Learning%2C%20satisfaction%2C%20and%20mistreatment%20during%20medical%20internship%3A%20a%20national%20survey%20of%20working%20conditions&journal=JAMA&doi=10.1001%2Fjama.279.15.1194&volume=279&publication_year=1998&author=Daugherty%2CSR&author=Baldwin%2CDC&author=Rowley%2CBD)\n32. Gettel, C. J. et al. Calculation of overall hospital quality star ratings with and without inclusion of the peer grouping step. *JAMA Netw. Open* **7** , e2411933 (2024). [Article](https://doi.org/10.1001%2Fjamanetworkopen.2024.11933) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=38753326) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11099678) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Calculation%20of%20overall%20hospital%20quality%20star%20ratings%20with%20and%20without%20inclusion%20of%20the%20peer%20grouping%20step&journal=JAMA%20Netw.%20Open&doi=10.1001%2Fjamanetworkopen.2024.11933&volume=7&publication_year=2024&author=Gettel%2CCJ)\n33. Martinez, K. A. et al. The association between physician race/ethnicity and patient satisfaction: an exploration in direct to consumer telemedicine. *J. Gen. Intern. Med.* **35** , 2600-2606 (2020). [Article](https://link.springer.com/doi/10.1007/s11606-020-06005-8) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32632788) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7459065) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20association%20between%20physician%20race%2Fethnicity%20and%20patient%20satisfaction%3A%20an%20exploration%20in%20direct%20to%20consumer%20telemedicine&journal=J.%20Gen.%20Intern.%20Med.&doi=10.1007%2Fs11606-020-06005-8&volume=35&pages=2600-2606&publication_year=2020&author=Martinez%2CKA)\n34. Siddiqui, Z. K. et al. Comparison of services available in 5-star and non-5-star patient experience hospitals. *JAMA Intern Med* **179** , 1429 (2019). [Article](https://doi.org/10.1001%2Fjamainternmed.2019.1285) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=31180447) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6563548) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Comparison%20of%20services%20available%20in%205-star%20and%20non%E2%80%935-star%20patient%20experience%20hospitals&journal=JAMA%20Intern%20Med&doi=10.1001%2Fjamainternmed.2019.1285&volume=179&publication_year=2019&author=Siddiqui%2CZK)\n35. Stokes, D. C. et al. Association between crowdsourced health care facility ratings and mortality in US counties. *JAMA Netw. Open* **4** , e2127799 (2021). [Article](https://doi.org/10.1001%2Fjamanetworkopen.2021.27799) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=34665240) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8527362) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Association%20between%20crowdsourced%20health%20care%20facility%20ratings%20and%20mortality%20in%20US%20counties&journal=JAMA%20Netw.%20Open&doi=10.1001%2Fjamanetworkopen.2021.27799&volume=4&publication_year=2021&author=Stokes%2CDC)\n36. Hayes, A. F. *Introduction to Mediation, Moderation, and Conditional Process Analysis: A Regression-Based Approach* . (Guilford Press, New York, 2018).\n\n[Download references](https://citation-needed.springer.com/v2/references/10.1038/s41746-025-01901-x?format=refman&flavour=references)\n\n## Acknowledgements\n\nThe authors acknowledge the support of Dr. Peter Greene and Dr. Jonathan Links, both of Johns Hopkins University, in facilitating survey distribution. This work was supported by a Johns Hopkins Discovery Award to Tinglong Dai, Risa M. Wolf, and Haiyang Yang for the project entitled \"Purposeful Design for AI-Augmented Healthcare: Harnessing Physician-in-the-Loop Systems to Improve the Patient Journey\". The funders had no role in study design, data collection, analysis, interpretation, manuscript preparation, or the decision to submit the work for publication.\n\n## Author information\n\n### Authors and Affiliations\n\n1. Carey Business School, Johns Hopkins University, Baltimore, MD, USA Haiyang Yang, Tinglong Dai, Yuna Nakayasu &amp; Risa M. Wolf\n2. Hopkins Business of Health Initiative, Johns Hopkins University, Washington, DC, USA Haiyang Yang, Tinglong Dai &amp; Risa M. Wolf\n3. School of Nursing, Johns Hopkins University, Baltimore, MD, USA Tinglong Dai\n4. Data Science and AI Institute, Johns Hopkins University, Baltimore, MD, USA Tinglong Dai\n5. School of Medicine, Johns Hopkins University, Baltimore, MD, USA Nestoras Mathioudakis, Amy M. Knight &amp; Risa M. Wolf\n6. Bloomberg School of Public Health, Johns Hopkins University, Baltimore, MD, USA Yuna Nakayasu\n\nAuthors\n\n1. Haiyang Yang [View author publications](/search?author=Haiyang%20Yang) Search author on: [PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Haiyang%20Yang) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Haiyang%20Yang%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n2. Tinglong Dai [View author publications](/search?author=Tinglong%20Dai) Search author on: [PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Tinglong%20Dai) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Tinglong%20Dai%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n3. Nestoras Mathioudakis [View author publications](/search?author=Nestoras%20Mathioudakis) Search author on: [PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Nestoras%20Mathioudakis) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Nestoras%20Mathioudakis%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n4. Amy M. Knight [View author publications](/search?author=Amy%20M.%20Knight) Search author on: [PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Amy%20M.%20Knight) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Amy%20M.%20Knight%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n5. Yuna Nakayasu [View author publications](/search?author=Yuna%20Nakayasu) Search author on: [PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Yuna%20Nakayasu) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Yuna%20Nakayasu%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n6. Risa M. Wolf [View author publications](/search?author=Risa%20M.%20Wolf) Search author on: [PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Risa%20M.%20Wolf) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Risa%20M.%20Wolf%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n\n### Contributions\n\nAll authors contributed to the concept and design of the study. Data acquisition, analysis, and interpretation were conducted by T.D., R.M.W., H.Y., A.M.K. and N.M. The manuscript was drafted by T.D., R.M.W., and H.Y. All authors critically revised the manuscript for important intellectual content. Statistical analysis was performed by H.Y. and N.M. Funding was obtained by T.D., R.M.W. and H.Y. Administrative, technical, or material support was provided by all authors. Supervision was provided by T.D. and R.M.W. All authors read and approved the final manuscript.\n\n### Corresponding authors\n\nCorrespondence to [Tinglong Dai](mailto:dai@jhu.edu) or [Risa M. Wolf](mailto:rwolf@jhu.edu) .\n\n## Ethics declarations\n\n### Competing interests\n\nT.D. declares no competing financial interests and a nonfinancial interest as a member of multiple study teams using LumineticsCore from Digital Diagnostics, and as co-lead of Johns Hopkins University's Bloomberg Distinguished Professorship Cluster on Global Advances in Medical Artificial Intelligence. T.D. is an Editor for *npj Digital Medicine* and played no role in the internal review or decision to publish this article. R.M.W. receives research support from Novo Nordisk, Lilly Diabetes, and Sanofi, unrelated to the current work. All other authors declare no competing interests.\n\n## Additional information\n\n**Publisher's note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\n## Supplementary information\n\n### [Supplementary Information](https://static-content.springer.com/esm/art%3A10.1038%2Fs41746-025-01901-x/MediaObjects/41746_2025_1901_MOESM1_ESM.docx)\n\n## Rights and permissions\n\n**Open Access** This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit [http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/) .\n\n[Reprints and permissions](https://s100.copyright.com/AppDispatchServlet?title=Peer%20perceptions%20of%20clinicians%20using%20generative%20AI%20in%20medical%20decision-making&author=Haiyang%20Yang%20et%20al&contentID=10.1038%2Fs41746-025-01901-x&copyright=The%20Author%28s%29&publication=2398-6352&publicationDate=2025-08-18&publisherName=SpringerNature&orderBeanReset=true&oa=CC%20BY)\n\n## About this article\n\nCheck for updates. Verify currency and authenticity via CrossMark\n\n<!-- image -->\n\n### Cite this article\n\nYang, H., Dai, T., Mathioudakis, N. *et al.* Peer perceptions of clinicians using generative AI in medical decision-making. *npj Digit. Med.* **8** , 530 (2025). https://doi.org/10.1038/s41746-025-01901-x\n\n[Download citation](https://citation-needed.springer.com/v2/references/10.1038/s41746-025-01901-x?format=refman&flavour=citation)\n\n- Received : 17 March 2025\n- Accepted : 21 July 2025\n- Published : 18 August 2025\n- Version of record : 18 August 2025\n- DOI : https://doi.org/10.1038/s41746-025-01901-x\n\n## This article is cited by\n\n- [Trust is all you Need: Reinforcing the Patient-physician Bond in Times of AI](https://doi.org/10.1007/s10916-025-02296-8) *Journal of Medical Systems* (2025)\n    - Florian Reis\n    - Moritz Reis\n    - Felix Balzer\n\n[Download PDF](/articles/s41746-025-01901-x.pdf)\n\nAdvertisement\n\n## Explore content\n\n- [Research articles](/npjdigitalmed/research-articles)\n- [Reviews &amp; Analysis](/npjdigitalmed/reviews-and-analysis)\n- [News &amp; Comment](/npjdigitalmed/news-and-comment)\n- [Collections](/npjdigitalmed/collections)\n\n- [Follow us on Twitter](https://twitter.com/npjDigitalMed)\n- [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id=41746)\n- [RSS feed](https://www.nature.com/npjdigitalmed.rss)\n\n## About the journal\n\n- [Aims and scope](/npjdigitalmed/aims)\n- [Content types](/npjdigitalmed/content-types)\n- [Journal Information](/npjdigitalmed/journal-information)\n- [About the Editors](/npjdigitalmed/editors)\n- [Contact](/npjdigitalmed/contact)\n- [Editorial policies](/npjdigitalmed/editorial-policies)\n- [Calls for Papers](/npjdigitalmed/calls-for-papers)\n- [Journal Metrics](/npjdigitalmed/journal-impact)\n- [About the Partner](/npjdigitalmed/partner)\n- [Open Access](/npjdigitalmed/open-access)\n- [Early Career Researcher Editorial Fellowship](/npjdigitalmed/editorial-fellowship)\n- [Editorial Team Vacancies](/npjdigitalmed/vacancies)\n- [News and Views Student Editor](/npjdigitalmed/news-and-views-student-editor)\n- [Communication Fellowship](/npjdigitalmed/communication-fellowship)\n\n## Publish with us\n\n- [For Authors and Referees](/npjdigitalmed/for-authors-and-referees)\n- [Language editing services](https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022)\n- [Open access funding](/npjdigitalmed/open-access-funding)\n- [Submit manuscript](https://submission.springernature.com/new-submission/41746/3)\n\n## Search\n\nSearch articles by subject, keyword or author\n\nShow results from\n\nAll journals\n\nThis journal\n\nSearch\n\n[Advanced search](/search/advanced)\n\n### Quick links\n\n- [Explore articles by subject](/subjects)\n- [Find a job](/naturecareers)\n- [Guide to authors](/authors/index.html)\n- [Editorial policies](/authors/editorial_policies)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n- [About us](https://www.nature.com/npg_/company_info/index.html)\n- [Press releases](https://www.nature.com/npg_/press_room/press_releases.html)\n- [Press office](https://press.nature.com/)\n- [Contact us](https://support.nature.com/support/home)\n\n### Discover content\n\n- [Journals A-Z](https://www.nature.com/siteindex)\n- [Articles by subject](https://www.nature.com/subjects)\n- [protocols.io](https://www.protocols.io/)\n- [Nature Index](https://www.natureindex.com/)\n\n### Publishing policies\n\n- [Nature portfolio policies](https://www.nature.com/authors/editorial_policies)\n- [Open access](https://www.nature.com/nature-research/open-access)\n\n### Author &amp; Researcher services\n\n- [Reprints &amp; permissions](https://www.nature.com/reprints)\n- [Research data](https://www.springernature.com/gp/authors/research-data)\n- [Language editing](https://authorservices.springernature.com/language-editing/)\n- [Scientific editing](https://authorservices.springernature.com/scientific-editing/)\n- [Nature Masterclasses](https://masterclasses.nature.com/)\n- [Research Solutions](https://solutions.springernature.com/)\n\n### Libraries &amp; institutions\n\n- [Librarian service &amp; tools](https://www.springernature.com/gp/librarians/tools-services)\n- [Librarian portal](https://www.springernature.com/gp/librarians/manage-your-account/librarianportal)\n- [Open research](https://www.nature.com/openresearch/about-open-access/information-for-institutions)\n- [Recommend to library](https://www.springernature.com/gp/librarians/recommend-to-your-library)\n\n### Advertising &amp; partnerships\n\n- [Advertising](https://partnerships.nature.com/product/digital-advertising/)\n- [Partnerships &amp; Services](https://partnerships.nature.com/)\n- [Media kits](https://partnerships.nature.com/media-kits/)\n- [Branded content](https://partnerships.nature.com/product/branded-content-native-advertising/)\n\n### Professional development\n\n- [Nature Awards](https://www.nature.com/immersive/natureawards/index.html)\n- [Nature Careers](https://www.nature.com/naturecareers/)\n- [Nature Conferences](https://conferences.nature.com/)\n\n### Regional websites\n\n- [Nature Africa](https://www.nature.com/natafrica)\n- [Nature China](http://www.naturechina.com/)\n- [Nature India](https://www.nature.com/nindia)\n- [Nature Japan](https://www.natureasia.com/ja-jp)\n- [Nature Middle East](https://www.nature.com/nmiddleeast)\n\n- [Privacy Policy](https://www.nature.com/info/privacy)\n- [Use of cookies](https://www.nature.com/info/cookies)\n- Your privacy choices/Manage cookies\n- [Legal notice](https://www.nature.com/info/legal-notice)\n- [Accessibility statement](https://www.nature.com/info/accessibility-statement)\n- [Terms &amp; Conditions](https://www.nature.com/info/terms-and-conditions)\n- [Your US state privacy rights](https://www.springernature.com/ccpa)\n\nSpringer Nature\n\n<!-- image -->\n\n\u00a9 2025 Springer Nature Limited\n\nClose banner\n\nClose\n\nNature Briefing\n\n<!-- image -->\n\nSign up for the *Nature Briefing* newsletter - what matters in science, free to your inbox daily.\n\nEmail address\n\nSign up\n\nI agree my information will be processed in accordance with the\n\n*Nature* and Springer Nature Limited [Privacy Policy](https://www.nature.com/info/privacy) .\n\nClose banner\n\nClose\n\nGet the most important science stories of the day, free in your inbox.\n\n[Sign up for Nature Briefing](https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner)"
    }
  ],
  "next_id": 3
}